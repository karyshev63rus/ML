{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='otus.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Николенко, Кадурин, Архангельская. **Глубокое обучение. Погружение в мир нейронных сетей**. Глава 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\"Мама мыла раму, и теперь она блестит\"  \n",
    "\"Мама мыла раму, и теперь она сильно устала\"  \n",
    "\n",
    "\"Кубок не помещался в чемодан, потому что он был слишком велик. Что именно было слишком велико, чемодан или кубок?\"\n",
    "\n",
    "http://commonsensereasoning.org/winograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. синтаксические задачи\n",
    "  * разметка по частям речи и по морфологическим признакам\n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация (?)\n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей\n",
    "  * разрешение смысла слов в заданном контексте (замок)\n",
    "  * построить синтаксическое дерево\n",
    "  * определение того, к каким другим объектам относится слово\n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "3. понимание и порождение текста (оценка качества?)\n",
    "  * порождение текста\n",
    "  * автоматическое реферирование\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "Косвенные задачи:\n",
    "  * описание изображения\n",
    "  * распознавание речи\n",
    "  \n",
    "**Задачи бизнеса**:\n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n",
    "  * ... (ваши варианты?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематическая модель автоматически определяет, к каким темам относится каждый документ из коллекции документов, а так же какие слова (термины) характеризуют каждую тему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d5/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 154.402s.\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " u\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.550s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.475s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.367s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.436s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: people just like time don say really know way things make think right said did want ve probably work years\n",
      "Topic #1: windows thanks using help need hi work know use looking mail software does used pc video available running info advance\n",
      "Topic #2: god does true read know say believe subject says religion mean question point jesus people book christian mind understand matter\n",
      "Topic #3: thanks know like interested mail just want new send edu list does bike thing email reply post wondering hear heard\n",
      "Topic #4: time new 10 year sale old offer 20 16 15 great 30 weeks good test model condition 11 14 power\n",
      "Topic #5: use number com government new university data states information talk phone right including security provide control following long used research\n",
      "Topic #6: edu try file soon remember problem com program hope mike space article wrong library short include win little couldn sun\n",
      "Topic #7: year world team game play won win games season maybe case second does did series playing nhl fact said points\n",
      "Topic #8: think don drive need hard make people mac read going pretty try sure order means trying apple case bit drives\n",
      "Topic #9: just good use way got like ll doesn want sure don doing thought does wrong right better make stuff speed\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 5.376s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.96604155,   4.3537397 ,  21.42539886, ...,   1.57926488,\n",
       "          1.33933502,   1.20988436],\n",
       "       [  0.48391034,   1.85845783,  14.04720958, ...,  74.59501615,\n",
       "         59.36116266,   0.27698642],\n",
       "       [  0.18708486,   0.13728929,   0.31409364, ...,   1.02679042,\n",
       "          2.56259123,   0.13662652],\n",
       "       ..., \n",
       "       [  3.22343848,  39.1368944 ,  11.24910558, ...,  23.37779481,\n",
       "          3.06315114,   0.15230766],\n",
       "       [  1.41871388,  47.53082031,  16.14390001, ...,  82.46751192,\n",
       "         16.51319941,  28.11660323],\n",
       "       [  4.02759659,   1.24781464,  13.26101699, ...,  29.0225105 ,\n",
       "          0.24834416,   0.13033208]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байес\n",
    "* Знаем метку каждого документа\n",
    "* У каждого документа только одна метка  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сделать, если нет информации о метках?\n",
    "#### Проблема кластеризации  \n",
    "Можно решать с помощью EM-алгоритма:\n",
    "* E-шаг - вычислить ожидания того, какой документ к какой теме относится\n",
    "* M-шаг - с помощью Наивного Байеса определить вероятности $p(w|t)$ при фиксированных метках\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-алгоритм\n",
    "\n",
    "Решает задачу кластеризации.  \n",
    "Подбирает некоторые параметры модели для данных в которых неизвестен ответ.  \n",
    "\n",
    "Expectation шаг:\n",
    "* зафиксировать параметры модели\n",
    "* посчитать значения скрытых переменных\n",
    "Maximization шаг:\n",
    "* зафиксировать скрытые переменные\n",
    "* посчитать параметры модели\n",
    "\n",
    "Повторять до сходимости.\n",
    "\n",
    "Есть математическое обоснование того, что метод сходится к локальному экстремуму, на каждом шаге значение функции правдоподобия не убывает (правдоподобие $p(\\theta | \\mathcal{X})$ - насколько правдоподобна модель при данных параметрах, насколдько она хорошо описывает данные)\n",
    "\n",
    "Частный случай EM-алгоритма - **k-means**.  \n",
    "Метки кластеров - скрытые переменные Z  (latent variables)  \n",
    "Параметры модели - центры кластеров  \n",
    "\n",
    "<img src=\"kmeans.png\">\n",
    "\n",
    "Еще вариант EM-алгоритма - разделение смеси гауссиан (Gaussian Mixture Model, GMM)\n",
    "\n",
    "Параметры модели - центр кластера и матрица ковариаций (здесь описывает форму могомерного нормального распраделения, или гауссианы)\n",
    "Скрытые переменные - вероятность пренадлежности к каждой гауссиане (метка кластера выбирается как наиболее вероятный кластер)\n",
    "\n",
    "\n",
    "<img src=\"gauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA\n",
    "\n",
    "Что если у каждого документа может быть много меток?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим модель:\n",
    "* Каждое слово в документе $d$ сгенерировано из некоторой темы $t \\in T$\n",
    "* Документ сгенерирован некоторым распределением над темами $p(t|d)$\n",
    "* Слово сгенерировано из темы (не из документа) $p(w|d, t) = p(w|d)$\n",
    "* Получаем правдоподобие: $$p(w|d) = \\sum_{t \\in T}p(w|t)p(t|d) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученная модель - probabilistic latent semantic analysis, pLSA, Вероятностный латентно-семантический анализ\n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D1%8B%D0%B9_%D0%BB%D0%B0%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%B5%D0%BC%D0%B0%D0%BD%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение:  \n",
    "Нам нужны величины:\n",
    "* $p(w|t)$ - вероятности слов в темах, обозначим $\\phi_{wt}$\n",
    "\n",
    "* $p(t|d)$ - вероятности тем в документах, обозначим $\\theta_{td}$\n",
    "\n",
    "E-шаг:\n",
    "* фиксируем $\\phi_{wt}$ и $\\theta_{td}$\n",
    "* вычисляем $$p(t|d,w) = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_{s \\in T}\\phi_{ws} \\theta_{sd}}$$ для всех тем, для каждого документа, для каждого термина\n",
    "* вычисляем количество терминов, которое генерируется в документе $d$ темой $t$ $$n_{dwt} = n_{dw}p(t|d,w)$$\n",
    "\n",
    "М-шаг:\n",
    "* по вычисленным $p(t|d,w)$ обновить приближения модели $\\phi_{wt}$ и $\\theta_{td}$\n",
    "* $$n_{wt} = \\sum_d n_{dwt}$$ $$n_{td} = \\sum_{w \\in d} n_{dwt}$$ $$n_t=\\sum_w n_{wt}$$\n",
    "* $$\\theta_{td} = \\frac{n_{td}}{n_d}$$ $$\\phi_{wt} = \\frac{n_{wt}}{n_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Можно не хранить матрицу $n_{dwt}$, а итерироваться по документам и суммировать $n_{wt}$ и $n_{td}$\n",
    "* Много локальных экстремумов\n",
    "* Много параметров, модель переобучается\n",
    "* Нужно достичь не локальный минимум, а добиться интерпретируемости - найти \"хороший\" минимум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае, чтобы улучшить pLSA, в логарифм правдоподобия добавляют регуляризацию:\n",
    "\n",
    "$$\\sum_{d \\in D} \\sum_{w \\in d} n_{dw} ln \\sum_{t \\in T} \\phi_{wt} \\theta_{td} + \\sum_i \\tau_i R_i(\\Phi, \\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если добавить априорное распределение - распределение дирехле, получим алгоритм LDA - Latent Dirichlet Allocation\n",
    "\n",
    "В итоге получаем \"хорошее\" интерпретируемое решение (лучше, чем с pLSA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один документ может содержать несколько тем.  \n",
    "Составляем иерархическую модель:  \n",
    "* первый уровень - смесь, компоненты которой отвечают за темы\n",
    "* второй уровень - мультиномиальная переменная с априорным распределением Дирихле, которая определяет \"распределение над темами\" в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение:\n",
    "* сэмплирование по Гибсу\n",
    "* online variational bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.076*\"sugar\" + 0.075*\"good\" + 0.075*\"health\"'), (1, u'0.084*\"sister\" + 0.084*\"father\" + 0.059*\"sugar\"'), (2, u'0.065*\"driving\" + 0.065*\"pressure\" + 0.064*\"doctor\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el50451398376321160488477889336\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el50451398376321160488477889336_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1], \"token.table\": {\"Topic\": [1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 3, 1, 1, 3, 3, 2, 3, 1, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1], \"Freq\": [0.8476064801216125, 0.8476835489273071, 0.847551167011261, 1.0316311120986938, 1.0316311120986938, 0.8476835489273071, 0.8476064801216125, 1.0316311120986938, 0.847551167011261, 0.6088731288909912, 0.6088731288909912, 1.1351406574249268, 0.7905506491661072, 0.847551167011261, 1.1351406574249268, 1.1351406574249268, 1.0316311120986938, 1.1351406574249268, 0.8476834893226624, 0.8476064801216125, 1.0316311120986938, 0.847551167011261, 0.847551167011261, 0.8476064801216125, 0.6086134314537048, 0.6086134314537048, 1.1351406574249268, 0.847551167011261, 0.847551167011261, 0.7905505895614624, 0.847551167011261, 0.8476064801216125, 1.0316312313079834, 0.8976562023162842, 0.4488281011581421, 1.0316311120986938, 0.8476064801216125, 0.847551167011261], \"Term\": [\"around\", \"bad\", \"better\", \"blood\", \"cause\", \"consume\", \"dance\", \"doctor\", \"drive\", \"driving\", \"driving\", \"expert\", \"father\", \"feel\", \"good\", \"health\", \"increased\", \"lifestyle\", \"like\", \"lot\", \"may\", \"never\", \"perform\", \"practice\", \"pressure\", \"pressure\", \"say\", \"school\", \"seems\", \"sister\", \"sometimes\", \"spends\", \"stress\", \"sugar\", \"sugar\", \"suggest\", \"time\", \"well\"]}, \"mdsDat\": {\"y\": [-0.029122684295965935, -0.03677091096911655, 0.0658935952650825], \"cluster\": [1, 1, 1], \"Freq\": [62.20137405395508, 22.339130401611328, 15.459491729736328], \"topics\": [1, 2, 3], \"x\": [0.07963407976351164, -0.07370155726591439, -0.0059325224975972]}, \"R\": 30, \"lambda.step\": 0.01, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Term\": [\"expert\", \"good\", \"lifestyle\", \"health\", \"say\", \"sugar\", \"suggest\", \"blood\", \"stress\", \"may\", \"cause\", \"doctor\", \"increased\", \"driving\", \"pressure\", \"father\", \"sister\", \"like\", \"bad\", \"consume\", \"practice\", \"dance\", \"spends\", \"time\", \"around\", \"lot\", \"school\", \"never\", \"feel\", \"sometimes\", \"father\", \"sister\", \"perform\", \"never\", \"drive\", \"well\", \"sometimes\", \"better\", \"feel\", \"seems\", \"school\", \"spends\", \"lot\", \"time\", \"around\", \"practice\", \"dance\", \"consume\", \"bad\", \"like\", \"sugar\", \"pressure\", \"driving\", \"good\", \"say\", \"health\", \"lifestyle\", \"expert\", \"doctor\", \"increased\", \"may\", \"increased\", \"doctor\", \"suggest\", \"blood\", \"stress\", \"cause\", \"driving\", \"pressure\", \"say\", \"health\", \"good\", \"expert\", \"lifestyle\", \"like\", \"bad\", \"consume\", \"around\", \"dance\", \"spends\", \"time\", \"lot\", \"practice\", \"school\", \"never\", \"better\", \"feel\", \"sometimes\", \"drive\", \"perform\", \"well\", \"sugar\", \"sister\", \"father\", \"seems\", \"health\", \"lifestyle\", \"say\", \"good\", \"expert\", \"sugar\", \"blood\", \"stress\", \"may\", \"increased\", \"doctor\", \"cause\", \"suggest\", \"consume\", \"bad\", \"like\", \"dance\", \"spends\", \"time\", \"lot\", \"around\", \"practice\", \"feel\", \"sometimes\", \"school\", \"perform\", \"never\", \"well\", \"drive\", \"seems\", \"driving\", \"pressure\", \"sister\", \"father\", \"better\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3573000133037567, 0.3573000133037567, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20350000262260437, 0.20329999923706055, 0.20329999923706055, 0.20329999923706055, 0.20329999923706055, 0.20329999923706055, 0.20329999923706055, 0.20309999585151672, 0.20309999585151672, 0.20309999585151672, 0.12370000034570694, -0.1315000057220459, -0.13349999487400055, -0.8895000219345093, -0.8895000219345093, -0.8895000219345093, -0.8895000219345093, -0.8895000219345093, -0.9853000044822693, -0.9853000044822693, 1.0500999689102173, 1.0500999689102173, 1.0500999689102173, 1.0500999689102173, 1.0500999689102173, 1.0500999689102173, 1.0500999689102173, 0.5292999744415283, 0.5264999866485596, -0.23929999768733978, -0.23929999768733978, -0.23929999768733978, -0.23929999768733978, -0.23929999768733978, -0.5303999781608582, -0.5303999781608582, -0.5303999781608582, -0.5307000279426575, -0.5307000279426575, -0.5307000279426575, -0.5307000279426575, -0.5307000279426575, -0.5307000279426575, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -0.5311999917030334, -1.1654000282287598, -1.2936999797821045, -1.2936999797821045, -0.5311999917030334, 1.30239999294281, 1.30239999294281, 1.30239999294281, 1.30239999294281, 1.30239999294281, 0.3817000091075897, -0.17829999327659607, -0.17829999327659607, -0.17829999327659607, -0.17829999327659607, -0.17829999327659607, -0.17829999327659607, -0.17829999327659607, -0.37290000915527344, -0.37290000915527344, -0.37290000915527344, -0.37389999628067017, -0.37389999628067017, -0.37389999628067017, -0.37389999628067017, -0.37389999628067017, -0.37389999628067017, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.37439998984336853, -0.7038000226020813, -0.7046999931335449, -1.1367000341415405, -1.1367000341415405, -0.37439998984336853], \"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.249397039413452, 2.249397039413452, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994821310043335, 0.8994820713996887, 0.899277925491333, 0.899277925491333, 0.899277925491333, 0.899277925491333, 0.899277925491333, 0.899277925491333, 0.8990191221237183, 0.8990190625190735, 0.8990190625190735, 1.568321704864502, 0.8961103558540344, 0.8939079642295837, 0.22513291239738464, 0.22513291239738464, 0.22513291239738464, 0.22513291239738464, 0.22513291239738464, 0.22510409355163574, 0.22510409355163574, 0.61885005235672, 0.61885005235672, 0.61885005235672, 0.61885005235672, 0.61885005235672, 0.61885005235672, 0.61885005235672, 0.6228644847869873, 0.6214191913604736, 0.15491348505020142, 0.15491345524787903, 0.15491345524787903, 0.15491345524787903, 0.15491345524787903, 0.15506020188331604, 0.15506012737751007, 0.1550600677728653, 0.15502050518989563, 0.15502050518989563, 0.15502050518989563, 0.15502050518989563, 0.15502050518989563, 0.15502050518989563, 0.15494957566261292, 0.15494954586029053, 0.15494953095912933, 0.15494953095912933, 0.15494953095912933, 0.15494953095912933, 0.15494953095912933, 0.15494951605796814, 0.15519262850284576, 0.15499107539653778, 0.15499098598957062, 0.15494951605796814, 0.5009016990661621, 0.5009016990661621, 0.5009016990661621, 0.5009016990661621, 0.5009016990661621, 0.504510223865509, 0.12538455426692963, 0.12538455426692963, 0.12538455426692963, 0.12538455426692963, 0.12538455426692963, 0.12538455426692963, 0.12538455426692963, 0.12560632824897766, 0.12560632824897766, 0.12560632824897766, 0.1254943162202835, 0.1254943162202835, 0.1254943162202835, 0.1254943162202835, 0.1254943162202835, 0.1254943162202835, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12543807923793793, 0.12560579180717468, 0.12554964423179626, 0.12549421191215515, 0.12549419701099396, 0.12543807923793793], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.5298821926116943, 2.5298824310302734, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1796854734420776, 1.1796854734420776, 1.1796855926513672, 2.228024482727051, 1.643079161643982, 1.642378330230713, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693386554718018, 0.9693387150764465, 1.642378330230713, 1.643079161643982, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 1.1796855926513672, 1.1796854734420776, 1.1796854734420776, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 2.228024482727051, 2.5298824310302734, 2.5298821926116943, 1.1798697710037231, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 0.8809480667114258, 2.228024482727051, 0.9693387150764465, 0.9693386554718018, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 0.9693387150764465, 1.1796854734420776, 1.1796854734420776, 1.1796855926513672, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1797927618026733, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.1798697710037231, 1.642378330230713, 1.643079161643982, 2.5298824310302734, 2.5298821926116943, 1.1798697710037231], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.4756999015808105, -2.4756999015808105, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3922998905181885, -3.3926000595092773, -3.3926000595092773, -3.3926000595092773, -3.3926000595092773, -3.3926000595092773, -3.3926000595092773, -3.392899990081787, -3.392899990081787, -3.392899990081787, -2.836400032043457, -3.3961000442504883, -3.3986001014709473, -4.777500152587891, -4.777500152587891, -4.777500152587891, -4.777500152587891, -4.777500152587891, -4.777599811553955, -4.777599811553955, -2.742300033569336, -2.742300033569336, -2.742300033569336, -2.742300033569336, -2.742300033569336, -2.742300033569336, -2.742300033569336, -2.73580002784729, -2.738100051879883, -4.127299785614014, -4.127299785614014, -4.127299785614014, -4.127299785614014, -4.127299785614014, -4.126299858093262, -4.126299858093262, -4.126299858093262, -4.1265997886657715, -4.1265997886657715, -4.1265997886657715, -4.1265997886657715, -4.1265997886657715, -4.1265997886657715, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.126999855041504, -4.125500202178955, -4.126800060272217, -4.126800060272217, -4.126999855041504, -2.585599899291992, -2.585599899291992, -2.585599899291992, -2.585599899291992, -2.585599899291992, -2.578399896621704, -3.970599889755249, -3.970599889755249, -3.970599889755249, -3.970599889755249, -3.970599889755249, -3.970599889755249, -3.970599889755249, -3.968899965286255, -3.968899965286255, -3.968899965286255, -3.9697000980377197, -3.9697000980377197, -3.9697000980377197, -3.9697000980377197, -3.9697000980377197, -3.9697000980377197, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.9702000617980957, -3.968899965286255, -3.9693000316619873, -3.9697000980377197, -3.9697000980377197, -3.9702000617980957]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el50451398376321160488477889336\", ldavis_el50451398376321160488477889336_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el50451398376321160488477889336\", ldavis_el50451398376321160488477889336_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el50451398376321160488477889336\", ldavis_el50451398376321160488477889336_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "1      62.201374        1       1  0.079634 -0.029123\n",
       "2      22.339130        1       2 -0.073702 -0.036771\n",
       "0      15.459492        1       3 -0.005933  0.065894, topic_info=     Category      Freq       Term     Total  loglift  logprob\n",
       "term                                                          \n",
       "0     Default  0.000000     expert  0.000000  30.0000  30.0000\n",
       "5     Default  0.000000       good  0.000000  29.0000  29.0000\n",
       "28    Default  0.000000  lifestyle  0.000000  28.0000  28.0000\n",
       "17    Default  0.000000     health  0.000000  27.0000  27.0000\n",
       "4     Default  0.000000        say  0.000000  26.0000  26.0000\n",
       "13    Default  2.000000      sugar  2.000000  25.0000  25.0000\n",
       "11    Default  0.000000    suggest  0.000000  24.0000  24.0000\n",
       "26    Default  0.000000      blood  0.000000  23.0000  23.0000\n",
       "24    Default  0.000000     stress  0.000000  22.0000  22.0000\n",
       "22    Default  0.000000        may  0.000000  21.0000  21.0000\n",
       "19    Default  0.000000      cause  0.000000  20.0000  20.0000\n",
       "9     Default  0.000000     doctor  0.000000  19.0000  19.0000\n",
       "7     Default  0.000000  increased  0.000000  18.0000  18.0000\n",
       "8     Default  1.000000    driving  1.000000  17.0000  17.0000\n",
       "25    Default  1.000000   pressure  1.000000  16.0000  16.0000\n",
       "12    Default  2.000000     father  2.000000  15.0000  15.0000\n",
       "27    Default  2.000000     sister  2.000000  14.0000  14.0000\n",
       "29    Default  1.000000       like  1.000000  13.0000  13.0000\n",
       "33    Default  1.000000        bad  1.000000  12.0000  12.0000\n",
       "1     Default  1.000000    consume  1.000000  11.0000  11.0000\n",
       "23    Default  1.000000   practice  1.000000  10.0000  10.0000\n",
       "2     Default  1.000000      dance  1.000000   9.0000   9.0000\n",
       "6     Default  1.000000     spends  1.000000   8.0000   8.0000\n",
       "34    Default  1.000000       time  1.000000   7.0000   7.0000\n",
       "21    Default  1.000000     around  1.000000   6.0000   6.0000\n",
       "18    Default  1.000000        lot  1.000000   5.0000   5.0000\n",
       "16    Default  1.000000     school  1.000000   4.0000   4.0000\n",
       "20    Default  1.000000      never  1.000000   3.0000   3.0000\n",
       "15    Default  1.000000       feel  1.000000   2.0000   2.0000\n",
       "30    Default  1.000000  sometimes  1.000000   1.0000   1.0000\n",
       "...       ...       ...        ...       ...      ...      ...\n",
       "13     Topic3  0.504510      sugar  2.228024   0.3817  -2.5784\n",
       "26     Topic3  0.125385      blood  0.969339  -0.1783  -3.9706\n",
       "24     Topic3  0.125385     stress  0.969339  -0.1783  -3.9706\n",
       "22     Topic3  0.125385        may  0.969339  -0.1783  -3.9706\n",
       "7      Topic3  0.125385  increased  0.969339  -0.1783  -3.9706\n",
       "9      Topic3  0.125385     doctor  0.969339  -0.1783  -3.9706\n",
       "19     Topic3  0.125385      cause  0.969339  -0.1783  -3.9706\n",
       "11     Topic3  0.125385    suggest  0.969339  -0.1783  -3.9706\n",
       "1      Topic3  0.125606    consume  1.179685  -0.3729  -3.9689\n",
       "33     Topic3  0.125606        bad  1.179685  -0.3729  -3.9689\n",
       "29     Topic3  0.125606       like  1.179686  -0.3729  -3.9689\n",
       "2      Topic3  0.125494      dance  1.179793  -0.3739  -3.9697\n",
       "6      Topic3  0.125494     spends  1.179793  -0.3739  -3.9697\n",
       "34     Topic3  0.125494       time  1.179793  -0.3739  -3.9697\n",
       "18     Topic3  0.125494        lot  1.179793  -0.3739  -3.9697\n",
       "21     Topic3  0.125494     around  1.179793  -0.3739  -3.9697\n",
       "23     Topic3  0.125494   practice  1.179793  -0.3739  -3.9697\n",
       "15     Topic3  0.125438       feel  1.179870  -0.3744  -3.9702\n",
       "30     Topic3  0.125438  sometimes  1.179870  -0.3744  -3.9702\n",
       "16     Topic3  0.125438     school  1.179870  -0.3744  -3.9702\n",
       "10     Topic3  0.125438    perform  1.179870  -0.3744  -3.9702\n",
       "20     Topic3  0.125438      never  1.179870  -0.3744  -3.9702\n",
       "31     Topic3  0.125438       well  1.179870  -0.3744  -3.9702\n",
       "32     Topic3  0.125438      drive  1.179870  -0.3744  -3.9702\n",
       "3      Topic3  0.125438      seems  1.179870  -0.3744  -3.9702\n",
       "8      Topic3  0.125606    driving  1.642378  -0.7038  -3.9689\n",
       "25     Topic3  0.125550   pressure  1.643079  -0.7047  -3.9693\n",
       "27     Topic3  0.125494     sister  2.529882  -1.1367  -3.9697\n",
       "12     Topic3  0.125494     father  2.529882  -1.1367  -3.9697\n",
       "14     Topic3  0.125438     better  1.179870  -0.3744  -3.9702\n",
       "\n",
       "[130 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "21        1  0.847606     around\n",
       "33        1  0.847684        bad\n",
       "14        1  0.847551     better\n",
       "26        2  1.031631      blood\n",
       "19        2  1.031631      cause\n",
       "1         1  0.847684    consume\n",
       "2         1  0.847606      dance\n",
       "9         2  1.031631     doctor\n",
       "32        1  0.847551      drive\n",
       "8         1  0.608873    driving\n",
       "8         2  0.608873    driving\n",
       "0         3  1.135141     expert\n",
       "12        1  0.790551     father\n",
       "15        1  0.847551       feel\n",
       "5         3  1.135141       good\n",
       "17        3  1.135141     health\n",
       "7         2  1.031631  increased\n",
       "28        3  1.135141  lifestyle\n",
       "29        1  0.847683       like\n",
       "18        1  0.847606        lot\n",
       "22        2  1.031631        may\n",
       "20        1  0.847551      never\n",
       "10        1  0.847551    perform\n",
       "23        1  0.847606   practice\n",
       "25        1  0.608613   pressure\n",
       "25        2  0.608613   pressure\n",
       "4         3  1.135141        say\n",
       "16        1  0.847551     school\n",
       "3         1  0.847551      seems\n",
       "27        1  0.790551     sister\n",
       "30        1  0.847551  sometimes\n",
       "6         1  0.847606     spends\n",
       "24        2  1.031631     stress\n",
       "13        1  0.897656      sugar\n",
       "13        3  0.448828      sugar\n",
       "11        2  1.031631    suggest\n",
       "34        1  0.847606       time\n",
       "31        1  0.847551       well, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/benhamner/nips-papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('papers.csv')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in ds['paper_text']]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'767',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'application',\n",
       " u'hisashi',\n",
       " u'suzuki',\n",
       " u'suguru',\n",
       " u'arimoto',\n",
       " u'osaka',\n",
       " u'university',\n",
       " u'toyonaka',\n",
       " u'osaka',\n",
       " u'560',\n",
       " u'japan',\n",
       " u'abstract',\n",
       " u'efficient',\n",
       " u'method',\n",
       " u'selforganizing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'proposed',\n",
       " u'together',\n",
       " u'application',\n",
       " u'robot',\n",
       " u'eyesight',\n",
       " u'system',\n",
       " u'proposed',\n",
       " u'database',\n",
       " u'associate',\n",
       " u'input',\n",
       " u'output',\n",
       " u'first',\n",
       " u'half',\n",
       " u'part',\n",
       " u'discussion',\n",
       " u'algorithm',\n",
       " u'selforganization',\n",
       " u'proposed',\n",
       " u'aspect',\n",
       " u'hardware',\n",
       " u'produce',\n",
       " u'new',\n",
       " u'style',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'latter',\n",
       " u'half',\n",
       " u'part',\n",
       " u'applicability',\n",
       " u'handwritten',\n",
       " u'letter',\n",
       " u'recognition',\n",
       " u'autonomous',\n",
       " u'mobile',\n",
       " u'robot',\n",
       " u'system',\n",
       " u'demonstrated',\n",
       " u'introduction',\n",
       " u'let',\n",
       " u'mapping',\n",
       " u'f',\n",
       " u'x',\n",
       " u'given',\n",
       " u'here',\n",
       " u'x',\n",
       " u'finite',\n",
       " u'infinite',\n",
       " u'set',\n",
       " u'another',\n",
       " u'finite',\n",
       " u'infinite',\n",
       " u'set',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'observes',\n",
       " u'set',\n",
       " u'pair',\n",
       " u'x',\n",
       " u'y',\n",
       " u'sampled',\n",
       " u'randomly',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'x',\n",
       " u'x',\n",
       " u'mean',\n",
       " u'cartesian',\n",
       " u'product',\n",
       " u'x',\n",
       " u'y',\n",
       " u'and',\n",
       " u'computes',\n",
       " u'estimate',\n",
       " u'j',\n",
       " u'x',\n",
       " u'f',\n",
       " u'make',\n",
       " u'small',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'measure',\n",
       " u'usually',\n",
       " u'say',\n",
       " u'that',\n",
       " u'faster',\n",
       " u'decrease',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'increase',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'better',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'however',\n",
       " u'expression',\n",
       " u'performance',\n",
       " u'incomplete',\n",
       " u'since',\n",
       " u'lack',\n",
       " u'consideration',\n",
       " u'candidate',\n",
       " u'j',\n",
       " u'j',\n",
       " u'assumed',\n",
       " u'preliminarily',\n",
       " u'then',\n",
       " u'find',\n",
       " u'good',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'clarify',\n",
       " u'conception',\n",
       " u'let',\n",
       " u'u',\n",
       " u'discus',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'and',\n",
       " u'let',\n",
       " u'u',\n",
       " u'advance',\n",
       " u'understanding',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'ordinary',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'assumes',\n",
       " u'equation',\n",
       " u'relating',\n",
       " u'x',\n",
       " u'y',\n",
       " u'parameter',\n",
       " u'indefinite',\n",
       " u'namely',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'equivalent',\n",
       " u'define',\n",
       " u'implicitly',\n",
       " u'set',\n",
       " u'f',\n",
       " u'candidate',\n",
       " u'f',\n",
       " u'subset',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'y',\n",
       " u'and',\n",
       " u'computes',\n",
       " u'value',\n",
       " u'parameter',\n",
       " u'based',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'call',\n",
       " u'type',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'defined',\n",
       " u'well',\n",
       " u'f',\n",
       " u'3',\n",
       " u'f',\n",
       " u'j',\n",
       " u'approach',\n",
       " u'f',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'alternative',\n",
       " u'case',\n",
       " u'however',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'remains',\n",
       " u'eternally',\n",
       " u'thus',\n",
       " u'problem',\n",
       " u'designing',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'return',\n",
       " u'find',\n",
       " u'proper',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'sense',\n",
       " u'hand',\n",
       " u'assumed',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'demanded',\n",
       " u'compact',\n",
       " u'possible',\n",
       " u'achieve',\n",
       " u'fast',\n",
       " u'learning',\n",
       " u'word',\n",
       " u'number',\n",
       " u'parameter',\n",
       " u'small',\n",
       " u'since',\n",
       " u'parameter',\n",
       " u'few',\n",
       " u'j',\n",
       " u'uniquely',\n",
       " u'determined',\n",
       " u'even',\n",
       " u'though',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'few',\n",
       " u'however',\n",
       " u'demand',\n",
       " u'proper',\n",
       " u'contradicts',\n",
       " u'compact',\n",
       " u'consequently',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'better',\n",
       " u'compactness',\n",
       " u'assumed',\n",
       " u'structure',\n",
       " u'proper',\n",
       " u'better',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'elementary',\n",
       " u'conception',\n",
       " u'design',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'1',\n",
       " u'universality',\n",
       " u'ordinary',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'suppose',\n",
       " u'sufficient',\n",
       " u'knowledge',\n",
       " u'f',\n",
       " u'given',\n",
       " u'though',\n",
       " u'j',\n",
       " u'unknown',\n",
       " u'case',\n",
       " u'comparatively',\n",
       " u'easy',\n",
       " u'find',\n",
       " u'proper',\n",
       " u'compact',\n",
       " u'structure',\n",
       " u'j',\n",
       " u'alternative',\n",
       " u'case',\n",
       " u'however',\n",
       " u'sometimes',\n",
       " u'difficult',\n",
       " u'possible',\n",
       " u'solution',\n",
       " u'give',\n",
       " u'compactness',\n",
       " u'assume',\n",
       " u'almighty',\n",
       " u'structure',\n",
       " u'cover',\n",
       " u'various',\n",
       " u'1',\n",
       " u'combination',\n",
       " u'orthogonal',\n",
       " u'base',\n",
       " u'infinite',\n",
       " u'dimension',\n",
       " u'structure',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'1',\n",
       " u'2',\n",
       " u'approximation',\n",
       " u'obtained',\n",
       " u'truncating',\n",
       " u'finitely',\n",
       " u'dimension',\n",
       " u'implementation',\n",
       " u'american',\n",
       " u'institute',\n",
       " u'physic',\n",
       " u'1988',\n",
       " u'768',\n",
       " u'main',\n",
       " u'topic',\n",
       " u'designing',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'establish',\n",
       " u'desirable',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'work',\n",
       " u'includes',\n",
       " u'developing',\n",
       " u'practical',\n",
       " u'procedure',\n",
       " u'compute',\n",
       " u'value',\n",
       " u'coefficient',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'discussion',\n",
       " u'flourishing',\n",
       " u'since',\n",
       " u'1980',\n",
       " u'many',\n",
       " u'efficient',\n",
       " u'method',\n",
       " u'proposed',\n",
       " u'recently',\n",
       " u'even',\n",
       " u'hardware',\n",
       " u'unit',\n",
       " u'computing',\n",
       " u'coefficient',\n",
       " u'parallel',\n",
       " u'speedup',\n",
       " u'sold',\n",
       " u'eg',\n",
       " u'anza',\n",
       " u'mark',\n",
       " u'iii',\n",
       " u'odyssey',\n",
       " u'e1',\n",
       " u'nevertheless',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'always',\n",
       " u'exists',\n",
       " u'danger',\n",
       " u'error',\n",
       " u'remaining',\n",
       " u'eternally',\n",
       " u'estimating',\n",
       " u'precisely',\n",
       " u'speaking',\n",
       " u'suppose',\n",
       " u'combination',\n",
       " u'base',\n",
       " u'finite',\n",
       " u'number',\n",
       " u'define',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'essentially',\n",
       " u'word',\n",
       " u'suppose',\n",
       " u'f',\n",
       " u'3',\n",
       " u'1',\n",
       " u'located',\n",
       " u'near',\n",
       " u'f',\n",
       " u'case',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'none',\n",
       " u'negligible',\n",
       " u'however',\n",
       " u'1',\n",
       " u'distant',\n",
       " u'f',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'never',\n",
       " u'becomes',\n",
       " u'negligible',\n",
       " u'indeed',\n",
       " u'many',\n",
       " u'research',\n",
       " u'report',\n",
       " u'following',\n",
       " u'situation',\n",
       " u'appears',\n",
       " u'1',\n",
       " u'complex',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'converges',\n",
       " u'value',\n",
       " u'0',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'decrease',\n",
       " u'hardly',\n",
       " u'even',\n",
       " u'though',\n",
       " u'dimension',\n",
       " u'heighten',\n",
       " u'property',\n",
       " u'sometimes',\n",
       " u'considerable',\n",
       " u'defect',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'recursi',\n",
       " u'type',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'founded',\n",
       " u'another',\n",
       " u'methodology',\n",
       " u'learning',\n",
       " u'follows',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'sample',\n",
       " u'set',\n",
       " u'fa',\n",
       " u'instead',\n",
       " u'notation',\n",
       " u'f',\n",
       " u'candidate',\n",
       " u'equal',\n",
       " u'set',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'y',\n",
       " u'observing',\n",
       " u'first',\n",
       " u'sample',\n",
       " u'xl',\n",
       " u'yl',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'fa',\n",
       " u'reduced',\n",
       " u'fi',\n",
       " u'ixt',\n",
       " u'yl',\n",
       " u'e',\n",
       " u'f',\n",
       " u'observing',\n",
       " u'second',\n",
       " u'sample',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'fl',\n",
       " u'reduced',\n",
       " u'f2',\n",
       " u'ixt',\n",
       " u'yl',\n",
       " u'ix2',\n",
       " u'y2',\n",
       " u'e',\n",
       " u'f',\n",
       " u'thus',\n",
       " u'candidate',\n",
       " u'set',\n",
       " u'f',\n",
       " u'becomes',\n",
       " u'gradually',\n",
       " u'small',\n",
       " u'observation',\n",
       " u'sample',\n",
       " u'proceeds',\n",
       " u'observing',\n",
       " u'isamples',\n",
       " u'write',\n",
       " u'one',\n",
       " u'likelihood',\n",
       " u'estimation',\n",
       " u'1',\n",
       " u'selected',\n",
       " u'fi',\n",
       " u'hence',\n",
       " u'contrarily',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'guarantee',\n",
       " u'surely',\n",
       " u'j',\n",
       " u'approach',\n",
       " u'1',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'observes',\n",
       " u'sample',\n",
       " u'x',\n",
       " u'yd',\n",
       " u'rewrite',\n",
       " u'value',\n",
       " u'1lxs',\n",
       " u'ix',\n",
       " u'x',\n",
       " u'correlated',\n",
       " u'sample',\n",
       " u'hence',\n",
       " u'type',\n",
       " u'architecture',\n",
       " u'composed',\n",
       " u'rule',\n",
       " u'rewriting',\n",
       " u'free',\n",
       " u'memory',\n",
       " u'space',\n",
       " u'architecture',\n",
       " u'form',\n",
       " u'naturally',\n",
       " u'kind',\n",
       " u'database',\n",
       " u'build',\n",
       " u'management',\n",
       " u'system',\n",
       " u'data',\n",
       " u'selforganizing',\n",
       " u'way',\n",
       " u'however',\n",
       " u'database',\n",
       " u'differs',\n",
       " u'ordinary',\n",
       " u'one',\n",
       " u'following',\n",
       " u'sense',\n",
       " u'record',\n",
       " u'sample',\n",
       " u'already',\n",
       " u'observed',\n",
       " u'computes',\n",
       " u'estimation',\n",
       " u'lx',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'call',\n",
       " u'database',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'first',\n",
       " u'subject',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'establish',\n",
       " u'rule',\n",
       " u'rewri',\n",
       " u'ting',\n",
       " u'purpose',\n",
       " u'adap',\n",
       " u'measure',\n",
       " u'called',\n",
       " u'dissimilari',\n",
       " u'ty',\n",
       " u'here',\n",
       " u'dissimilari',\n",
       " u'ty',\n",
       " u'mean',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'x',\n",
       " u'x',\n",
       " u'real',\n",
       " u'o',\n",
       " u'x',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'x',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'0',\n",
       " u'whenever',\n",
       " u'lx',\n",
       " u'x',\n",
       " u'however',\n",
       " u'necessarily',\n",
       " u'defined',\n",
       " u'single',\n",
       " u'formula',\n",
       " u'definable',\n",
       " u'with',\n",
       " u'example',\n",
       " u'collection',\n",
       " u'rule',\n",
       " u'written',\n",
       " u'form',\n",
       " u'if',\n",
       " u'then',\n",
       " u'dissimilarity',\n",
       " u'defines',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'locally',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'hence',\n",
       " u'even',\n",
       " u'though',\n",
       " u'knowledge',\n",
       " u'f',\n",
       " u'imperfect',\n",
       " u'reflect',\n",
       " u'heuristic',\n",
       " u'way',\n",
       " u'hence',\n",
       " u'contrarily',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'possible',\n",
       " u'accelerate',\n",
       " u'speed',\n",
       " u'learning',\n",
       " u'establishing',\n",
       " u'well',\n",
       " u'especially',\n",
       " u'easily',\n",
       " u'find',\n",
       " u'simple',\n",
       " u'd',\n",
       " u'l',\n",
       " u'process',\n",
       " u'analogically',\n",
       " u'information',\n",
       " u'like',\n",
       " u'human',\n",
       " u'see',\n",
       " u'application',\n",
       " u'paper',\n",
       " u'and',\n",
       " u's',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'show',\n",
       " u'strongly',\n",
       " u'effectiveness',\n",
       " u'denote',\n",
       " u'sequence',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'xl',\n",
       " u'yd',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'one',\n",
       " u'simplest',\n",
       " u'construction',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'observing',\n",
       " u'isamples',\n",
       " u'i',\n",
       " u'12',\n",
       " u'follows',\n",
       " u'i',\n",
       " u'i',\n",
       " u'algorithm',\n",
       " u'1',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'let',\n",
       " u'empty',\n",
       " u'set',\n",
       " u'every',\n",
       " u'12',\n",
       " u'let',\n",
       " u'ilx',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'equal',\n",
       " u'y',\n",
       " u'xy',\n",
       " u'e',\n",
       " u'sl',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'min',\n",
       " u'yest',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'furthermore',\n",
       " u'add',\n",
       " u'x',\n",
       " u'y',\n",
       " u'sl',\n",
       " u'produce',\n",
       " u'sa',\n",
       " u'ie',\n",
       " u's',\n",
       " u'sl',\n",
       " u'u',\n",
       " u'x',\n",
       " u'1',\n",
       " u'yn',\n",
       " u'769',\n",
       " u'another',\n",
       " u'version',\n",
       " u'improved',\n",
       " u'economize',\n",
       " u'memory',\n",
       " u'follows',\n",
       " u'algorithm',\n",
       " u'2',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'let',\n",
       " u'composed',\n",
       " u'arbitrary',\n",
       " u'element',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'every',\n",
       " u'12',\n",
       " u'let',\n",
       " u'iilex',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'equal',\n",
       " u'y',\n",
       " u'x',\n",
       " u'y',\n",
       " u'e',\n",
       " u'sil',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'min',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'iiesl',\n",
       " u'furthermore',\n",
       " u'iilxi',\n",
       " u'yi',\n",
       " u'let',\n",
       " u'si',\n",
       " u'sil',\n",
       " u'add',\n",
       " u'xi',\n",
       " u'yi',\n",
       " u'sil',\n",
       " u'produce',\n",
       " u'si',\n",
       " u'ie',\n",
       " u'si',\n",
       " u'sil',\n",
       " u'u',\n",
       " u'xi',\n",
       " u'yi',\n",
       " u'either',\n",
       " u'construction',\n",
       " u'ii',\n",
       " u'approach',\n",
       " u'f',\n",
       " u'increase',\n",
       " u'however',\n",
       " u'computation',\n",
       " u'time',\n",
       " u'grows',\n",
       " u'proportionally',\n",
       " u'size',\n",
       " u'si',\n",
       " u'second',\n",
       " u'subject',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'addressing',\n",
       " u'rule',\n",
       " u'employ',\n",
       " u'economize',\n",
       " u'computation',\n",
       " u'time',\n",
       " u'subsequent',\n",
       " u'chapter',\n",
       " u'construction',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'purpose',\n",
       " u'proposed',\n",
       " u'manages',\n",
       " u'data',\n",
       " u'form',\n",
       " u'binary',\n",
       " u'tree',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'given',\n",
       " u'sample',\n",
       " u'sequence',\n",
       " u'xl',\n",
       " u'yl',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'algorithm',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'follows',\n",
       " u'algorithm',\n",
       " u'3',\n",
       " u'step',\n",
       " u'iinitialization',\n",
       " u'let',\n",
       " u'xroot',\n",
       " u'yroot',\n",
       " u'xl',\n",
       " u'yd',\n",
       " u'here',\n",
       " u'x',\n",
       " u'y',\n",
       " u'variable',\n",
       " u'assigned',\n",
       " u'respective',\n",
       " u'node',\n",
       " u'memorize',\n",
       " u'data',\n",
       " u'furthermore',\n",
       " u'let',\n",
       " u'1',\n",
       " u'step',\n",
       " u'2',\n",
       " u'increase',\n",
       " u'1',\n",
       " u'put',\n",
       " u'x',\n",
       " u'in',\n",
       " u'reset',\n",
       " u'pointer',\n",
       " u'n',\n",
       " u'root',\n",
       " u'repeat',\n",
       " u'following',\n",
       " u'n',\n",
       " u'arrives',\n",
       " u'terminal',\n",
       " u'node',\n",
       " u'ie',\n",
       " u'leaf',\n",
       " u'notation',\n",
       " u'nand',\n",
       " u'dxt',\n",
       " u'xn',\n",
       " u'let',\n",
       " u'n',\n",
       " u'n',\n",
       " u'mean',\n",
       " u'descendant',\n",
       " u'node',\n",
       " u'n',\n",
       " u'n',\n",
       " u'otherwise',\n",
       " u'let',\n",
       " u'n',\n",
       " u'n',\n",
       " u'dx',\n",
       " u'rn',\n",
       " u'step',\n",
       " u'3',\n",
       " u'display',\n",
       " u'yin',\n",
       " u'related',\n",
       " u'information',\n",
       " u'next',\n",
       " u'put',\n",
       " u'y',\n",
       " u'in',\n",
       " u'yin',\n",
       " u'y',\n",
       " u'back',\n",
       " u'step',\n",
       " u'2',\n",
       " u'otherwise',\n",
       " u'first',\n",
       " u'establish',\n",
       " u'new',\n",
       " u'descendant',\n",
       " u'node',\n",
       " u'n',\n",
       " u'n',\n",
       " u'secondly',\n",
       " u'let',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xt',\n",
       " u'y',\n",
       " u'2',\n",
       " u'3',\n",
       " u'finally',\n",
       " u'back',\n",
       " u'step',\n",
       " u'2',\n",
       " u'here',\n",
       " u'loop',\n",
       " u'step',\n",
       " u'23',\n",
       " u'stopped',\n",
       " u'time',\n",
       " u'also',\n",
       " u'continued',\n",
       " u'now',\n",
       " u'suppose',\n",
       " u'gate',\n",
       " u'element',\n",
       " u'namely',\n",
       " u'artificial',\n",
       " u'synapsis',\n",
       " u'play',\n",
       " u'role',\n",
       " u'branching',\n",
       " u'prepared',\n",
       " u'then',\n",
       " u'obtain',\n",
       " u'new',\n",
       " u'style',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'gate',\n",
       " u'element',\n",
       " u'randomly',\n",
       " u'connected',\n",
       " u'algorithm',\n",
       " u'letter',\n",
       " u'recognition',\n",
       " u'recen',\n",
       " u'tly',\n",
       " u'vertical',\n",
       " u'slitting',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'typographic',\n",
       " u'english',\n",
       " u'letters3',\n",
       " u'elastic',\n",
       " u'matching',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'hand',\n",
       " u'written',\n",
       " u'discrete',\n",
       " u'english',\n",
       " u'letters4',\n",
       " u'global',\n",
       " u'training',\n",
       " u'fuzzy',\n",
       " u'logic',\n",
       " u'search',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'chinese',\n",
       " u'character',\n",
       " u'written',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5),\n",
       " (1, 22),\n",
       " (2, 1),\n",
       " (3, 3),\n",
       " (4, 3),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 4),\n",
       " (9, 1),\n",
       " (10, 2),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 1),\n",
       " (16, 4),\n",
       " (17, 9),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 11),\n",
       " (22, 2),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 4),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 12),\n",
       " (34, 2),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 4),\n",
       " (46, 2),\n",
       " (47, 3),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 7),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 7),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 3),\n",
       " (74, 2),\n",
       " (75, 1),\n",
       " (76, 2),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 2),\n",
       " (81, 1),\n",
       " (82, 6),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 2),\n",
       " (87, 1),\n",
       " (88, 6),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 3),\n",
       " (92, 1),\n",
       " (93, 2),\n",
       " (94, 2),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 2),\n",
       " (99, 4),\n",
       " (100, 8),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 2),\n",
       " (104, 2),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 8),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 6),\n",
       " (114, 2),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 5),\n",
       " (119, 3),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 2),\n",
       " (125, 7),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 3),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 13),\n",
       " (137, 1),\n",
       " (138, 3),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 2),\n",
       " (143, 2),\n",
       " (144, 1),\n",
       " (145, 4),\n",
       " (146, 5),\n",
       " (147, 4),\n",
       " (148, 6),\n",
       " (149, 2),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 2),\n",
       " (153, 2),\n",
       " (154, 4),\n",
       " (155, 1),\n",
       " (156, 3),\n",
       " (157, 1),\n",
       " (158, 3),\n",
       " (159, 9),\n",
       " (160, 1),\n",
       " (161, 2),\n",
       " (162, 6),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 2),\n",
       " (170, 5),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 2),\n",
       " (174, 1),\n",
       " (175, 11),\n",
       " (176, 4),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 7),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 4),\n",
       " (186, 1),\n",
       " (187, 3),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 2),\n",
       " (196, 1),\n",
       " (197, 2),\n",
       " (198, 1),\n",
       " (199, 2),\n",
       " (200, 2),\n",
       " (201, 2),\n",
       " (202, 3),\n",
       " (203, 2),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 2),\n",
       " (207, 1),\n",
       " (208, 3),\n",
       " (209, 1),\n",
       " (210, 2),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 3),\n",
       " (214, 2),\n",
       " (215, 2),\n",
       " (216, 1),\n",
       " (217, 3),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 2),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 3),\n",
       " (228, 3),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 3),\n",
       " (233, 1),\n",
       " (234, 2),\n",
       " (235, 2),\n",
       " (236, 4),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 4),\n",
       " (240, 2),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 2),\n",
       " (244, 3),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 6),\n",
       " (249, 1),\n",
       " (250, 7),\n",
       " (251, 19),\n",
       " (252, 1),\n",
       " (253, 2),\n",
       " (254, 2),\n",
       " (255, 1),\n",
       " (256, 14),\n",
       " (257, 3),\n",
       " (258, 2),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 2),\n",
       " (264, 1),\n",
       " (265, 2),\n",
       " (266, 1),\n",
       " (267, 2),\n",
       " (268, 2),\n",
       " (269, 2),\n",
       " (270, 2),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 3),\n",
       " (278, 1),\n",
       " (279, 2),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 2),\n",
       " (284, 1),\n",
       " (285, 2),\n",
       " (286, 1),\n",
       " (287, 2),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 3),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 3),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 9),\n",
       " (301, 1),\n",
       " (302, 17),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 2),\n",
       " (307, 1),\n",
       " (308, 2),\n",
       " (309, 2),\n",
       " (310, 1),\n",
       " (311, 2),\n",
       " (312, 3),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 3),\n",
       " (319, 1),\n",
       " (320, 2),\n",
       " (321, 3),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 8),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 4),\n",
       " (328, 1),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 8),\n",
       " (333, 4),\n",
       " (334, 2),\n",
       " (335, 2),\n",
       " (336, 2),\n",
       " (337, 4),\n",
       " (338, 4),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 7),\n",
       " (342, 1),\n",
       " (343, 1),\n",
       " (344, 2),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 2),\n",
       " (349, 4),\n",
       " (350, 3),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 2),\n",
       " (355, 21),\n",
       " (356, 1),\n",
       " (357, 3),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 2),\n",
       " (368, 2),\n",
       " (369, 21),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 5),\n",
       " (374, 4),\n",
       " (375, 1),\n",
       " (376, 6),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 7),\n",
       " (382, 4),\n",
       " (383, 3),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 2),\n",
       " (390, 3),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 3),\n",
       " (394, 2),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (397, 2),\n",
       " (398, 1),\n",
       " (399, 2),\n",
       " (400, 3),\n",
       " (401, 3),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 2),\n",
       " (409, 2),\n",
       " (410, 1),\n",
       " (411, 2),\n",
       " (412, 1),\n",
       " (413, 3),\n",
       " (414, 2),\n",
       " (415, 2),\n",
       " (416, 1),\n",
       " (417, 7),\n",
       " (418, 6),\n",
       " (419, 2),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 10),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 6),\n",
       " (429, 1),\n",
       " (430, 8),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 4),\n",
       " (435, 6),\n",
       " (436, 2),\n",
       " (437, 2),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 1),\n",
       " (441, 1),\n",
       " (442, 1),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 23),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 2),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 2),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 4),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 2),\n",
       " (458, 7),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 3),\n",
       " (462, 3),\n",
       " (463, 3),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (468, 2),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 4),\n",
       " (472, 1),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 2),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 2),\n",
       " (482, 2),\n",
       " (483, 1),\n",
       " (484, 1),\n",
       " (485, 2),\n",
       " (486, 15),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 1),\n",
       " (490, 2),\n",
       " (491, 2),\n",
       " (492, 2),\n",
       " (493, 2),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 2),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 3),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 3),\n",
       " (506, 1),\n",
       " (507, 17),\n",
       " (508, 2),\n",
       " (509, 1),\n",
       " (510, 2),\n",
       " (511, 1),\n",
       " (512, 18),\n",
       " (513, 11),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 4),\n",
       " (517, 2),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 3),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 1),\n",
       " (531, 1),\n",
       " (532, 2),\n",
       " (533, 2),\n",
       " (534, 2),\n",
       " (535, 1),\n",
       " (536, 11),\n",
       " (537, 1),\n",
       " (538, 1),\n",
       " (539, 1),\n",
       " (540, 2),\n",
       " (541, 1),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 2),\n",
       " (545, 4),\n",
       " (546, 1),\n",
       " (547, 2),\n",
       " (548, 1),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 4),\n",
       " (552, 6),\n",
       " (553, 1),\n",
       " (554, 3),\n",
       " (555, 9),\n",
       " (556, 2),\n",
       " (557, 2),\n",
       " (558, 1),\n",
       " (559, 5),\n",
       " (560, 1),\n",
       " (561, 2),\n",
       " (562, 1),\n",
       " (563, 5),\n",
       " (564, 2),\n",
       " (565, 2),\n",
       " (566, 4),\n",
       " (567, 1),\n",
       " (568, 2),\n",
       " (569, 13),\n",
       " (570, 4),\n",
       " (571, 2),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 2),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 2),\n",
       " (581, 8),\n",
       " (582, 9),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 4),\n",
       " (586, 1),\n",
       " (587, 7),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 2),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (594, 3),\n",
       " (595, 1),\n",
       " (596, 14),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 1),\n",
       " (600, 5),\n",
       " (601, 2),\n",
       " (602, 4),\n",
       " (603, 7),\n",
       " (604, 1),\n",
       " (605, 2),\n",
       " (606, 1),\n",
       " (607, 1),\n",
       " (608, 1),\n",
       " (609, 7),\n",
       " (610, 2),\n",
       " (611, 2),\n",
       " (612, 10),\n",
       " (613, 1),\n",
       " (614, 3),\n",
       " (615, 1),\n",
       " (616, 2),\n",
       " (617, 2),\n",
       " (618, 1),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 2),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 8),\n",
       " (626, 3),\n",
       " (627, 1),\n",
       " (628, 1),\n",
       " (629, 1),\n",
       " (630, 3),\n",
       " (631, 3),\n",
       " (632, 1),\n",
       " (633, 1),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 1),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 1),\n",
       " (640, 2),\n",
       " (641, 1),\n",
       " (642, 1),\n",
       " (643, 12),\n",
       " (644, 1),\n",
       " (645, 15),\n",
       " (646, 1),\n",
       " (647, 1),\n",
       " (648, 3),\n",
       " (649, 10),\n",
       " (650, 2),\n",
       " (651, 1),\n",
       " (652, 2),\n",
       " (653, 1),\n",
       " (654, 2),\n",
       " (655, 1),\n",
       " (656, 2),\n",
       " (657, 1),\n",
       " (658, 1),\n",
       " (659, 1),\n",
       " (660, 1),\n",
       " (661, 1),\n",
       " (662, 6),\n",
       " (663, 1),\n",
       " (664, 1),\n",
       " (665, 3),\n",
       " (666, 2),\n",
       " (667, 2),\n",
       " (668, 3),\n",
       " (669, 1),\n",
       " (670, 2),\n",
       " (671, 4),\n",
       " (672, 1),\n",
       " (673, 1),\n",
       " (674, 7),\n",
       " (675, 1),\n",
       " (676, 4),\n",
       " (677, 2),\n",
       " (678, 1),\n",
       " (679, 1),\n",
       " (680, 3),\n",
       " (681, 1),\n",
       " (682, 1),\n",
       " (683, 2),\n",
       " (684, 1),\n",
       " (685, 9),\n",
       " (686, 1),\n",
       " (687, 1),\n",
       " (688, 4),\n",
       " (689, 1),\n",
       " (690, 1),\n",
       " (691, 1),\n",
       " (692, 2),\n",
       " (693, 2),\n",
       " (694, 1),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (697, 11),\n",
       " (698, 4),\n",
       " (699, 3),\n",
       " (700, 1),\n",
       " (701, 1),\n",
       " (702, 1),\n",
       " (703, 4),\n",
       " (704, 1),\n",
       " (705, 2),\n",
       " (706, 1),\n",
       " (707, 1),\n",
       " (708, 1),\n",
       " (709, 1),\n",
       " (710, 7),\n",
       " (711, 1),\n",
       " (712, 1),\n",
       " (713, 1),\n",
       " (714, 2),\n",
       " (715, 3),\n",
       " (716, 1),\n",
       " (717, 1),\n",
       " (718, 1),\n",
       " (719, 1),\n",
       " (720, 1),\n",
       " (721, 1),\n",
       " (722, 1),\n",
       " (723, 1),\n",
       " (724, 2),\n",
       " (725, 3),\n",
       " (726, 1),\n",
       " (727, 2),\n",
       " (728, 1),\n",
       " (729, 1),\n",
       " (730, 1),\n",
       " (731, 1),\n",
       " (732, 1),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 1),\n",
       " (736, 1),\n",
       " (737, 1),\n",
       " (738, 2),\n",
       " (739, 23),\n",
       " (740, 5),\n",
       " (741, 2),\n",
       " (742, 1),\n",
       " (743, 1),\n",
       " (744, 6),\n",
       " (745, 6),\n",
       " (746, 1),\n",
       " (747, 1),\n",
       " (748, 3),\n",
       " (749, 1),\n",
       " (750, 1),\n",
       " (751, 18),\n",
       " (752, 1),\n",
       " (753, 2),\n",
       " (754, 1),\n",
       " (755, 1),\n",
       " (756, 1),\n",
       " (757, 3),\n",
       " (758, 1),\n",
       " (759, 4),\n",
       " (760, 2),\n",
       " (761, 1),\n",
       " (762, 3),\n",
       " (763, 1),\n",
       " (764, 1),\n",
       " (765, 1),\n",
       " (766, 7),\n",
       " (767, 3),\n",
       " (768, 2),\n",
       " (769, 4),\n",
       " (770, 9),\n",
       " (771, 1),\n",
       " (772, 5),\n",
       " (773, 1),\n",
       " (774, 1),\n",
       " (775, 4),\n",
       " (776, 4),\n",
       " (777, 3),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (780, 1),\n",
       " (781, 5),\n",
       " (782, 1),\n",
       " (783, 1),\n",
       " (784, 1),\n",
       " (785, 3),\n",
       " (786, 2),\n",
       " (787, 1),\n",
       " (788, 1),\n",
       " (789, 1),\n",
       " (790, 3),\n",
       " (791, 1),\n",
       " (792, 1),\n",
       " (793, 2),\n",
       " (794, 1),\n",
       " (795, 2),\n",
       " (796, 1),\n",
       " (797, 2),\n",
       " (798, 1),\n",
       " (799, 1),\n",
       " (800, 2),\n",
       " (801, 1),\n",
       " (802, 1),\n",
       " (803, 1),\n",
       " (804, 3),\n",
       " (805, 1),\n",
       " (806, 1),\n",
       " (807, 1),\n",
       " (808, 1),\n",
       " (809, 6),\n",
       " (810, 2),\n",
       " (811, 1),\n",
       " (812, 1),\n",
       " (813, 1),\n",
       " (814, 1),\n",
       " (815, 13),\n",
       " (816, 3),\n",
       " (817, 1),\n",
       " (818, 1),\n",
       " (819, 4),\n",
       " (820, 1),\n",
       " (821, 1),\n",
       " (822, 1),\n",
       " (823, 1),\n",
       " (824, 1),\n",
       " (825, 1),\n",
       " (826, 1),\n",
       " (827, 1),\n",
       " (828, 1),\n",
       " (829, 1),\n",
       " (830, 1),\n",
       " (831, 1),\n",
       " (832, 1),\n",
       " (833, 2),\n",
       " (834, 1),\n",
       " (835, 1),\n",
       " (836, 4),\n",
       " (837, 1),\n",
       " (838, 1),\n",
       " (839, 1),\n",
       " (840, 1),\n",
       " (841, 1),\n",
       " (842, 1),\n",
       " (843, 1),\n",
       " (844, 8),\n",
       " (845, 4),\n",
       " (846, 1),\n",
       " (847, 3),\n",
       " (848, 1),\n",
       " (849, 4),\n",
       " (850, 1),\n",
       " (851, 1),\n",
       " (852, 1),\n",
       " (853, 1),\n",
       " (854, 1),\n",
       " (855, 1),\n",
       " (856, 1),\n",
       " (857, 1),\n",
       " (858, 2),\n",
       " (859, 6),\n",
       " (860, 1),\n",
       " (861, 1),\n",
       " (862, 4),\n",
       " (863, 1),\n",
       " (864, 6),\n",
       " (865, 9),\n",
       " (866, 1),\n",
       " (867, 1),\n",
       " (868, 1),\n",
       " (869, 2),\n",
       " (870, 1),\n",
       " (871, 1),\n",
       " (872, 1),\n",
       " (873, 2),\n",
       " (874, 2),\n",
       " (875, 1),\n",
       " (876, 1),\n",
       " (877, 5),\n",
       " (878, 3),\n",
       " (879, 1),\n",
       " (880, 1),\n",
       " (881, 1),\n",
       " (882, 1),\n",
       " (883, 2),\n",
       " (884, 1),\n",
       " (885, 2),\n",
       " (886, 14),\n",
       " (887, 2),\n",
       " (888, 1),\n",
       " (889, 6),\n",
       " (890, 1),\n",
       " (891, 1),\n",
       " (892, 1),\n",
       " (893, 2),\n",
       " (894, 1),\n",
       " (895, 1),\n",
       " (896, 1),\n",
       " (897, 2),\n",
       " (898, 1),\n",
       " (899, 2),\n",
       " (900, 1),\n",
       " (901, 1),\n",
       " (902, 2),\n",
       " (903, 6),\n",
       " (904, 1),\n",
       " (905, 3),\n",
       " (906, 2),\n",
       " (907, 1),\n",
       " (908, 1),\n",
       " (909, 1),\n",
       " (910, 2),\n",
       " (911, 1),\n",
       " (912, 1),\n",
       " (913, 2),\n",
       " (914, 2),\n",
       " (915, 1),\n",
       " (916, 1),\n",
       " (917, 1),\n",
       " (918, 3),\n",
       " (919, 1),\n",
       " (920, 3),\n",
       " (921, 1),\n",
       " (922, 1),\n",
       " (923, 3),\n",
       " (924, 1),\n",
       " (925, 1),\n",
       " (926, 1),\n",
       " (927, 2),\n",
       " (928, 10),\n",
       " (929, 1),\n",
       " (930, 1),\n",
       " (931, 2),\n",
       " (932, 1),\n",
       " (933, 1),\n",
       " (934, 5),\n",
       " (935, 1),\n",
       " (936, 1),\n",
       " (937, 1),\n",
       " (938, 3),\n",
       " (939, 91),\n",
       " (940, 3),\n",
       " (941, 2),\n",
       " (942, 1),\n",
       " (943, 4),\n",
       " (944, 4),\n",
       " (945, 1),\n",
       " (946, 1),\n",
       " (947, 1),\n",
       " (948, 25),\n",
       " (949, 4),\n",
       " (950, 3),\n",
       " (951, 1),\n",
       " (952, 3),\n",
       " (953, 5),\n",
       " (954, 4),\n",
       " (955, 1),\n",
       " (956, 1),\n",
       " (957, 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.021*\"network\" + 0.009*\"neural\" + 0.008*\"1\" + 0.007*\"input\" + 0.007*\"model\" + 0.005*\"output\" + 0.005*\"unit\" + 0.005*\"time\" + 0.005*\"2\" + 0.004*\"learning\"')\n",
      "(1, u'0.016*\"1\" + 0.012*\"2\" + 0.008*\"model\" + 0.008*\"x\" + 0.006*\"0\" + 0.006*\"p\" + 0.006*\"algorithm\" + 0.006*\"graph\" + 0.005*\"r\" + 0.005*\"n\"')\n",
      "(2, u'0.013*\"image\" + 0.013*\"model\" + 0.010*\"1\" + 0.006*\"2\" + 0.005*\"x\" + 0.005*\"3\" + 0.005*\"feature\" + 0.004*\"a\" + 0.004*\"using\" + 0.004*\"object\"')\n",
      "(3, u'0.014*\"1\" + 0.012*\"model\" + 0.008*\"data\" + 0.008*\"x\" + 0.007*\"learning\" + 0.005*\"distribution\" + 0.005*\"2\" + 0.005*\"k\" + 0.005*\"0\" + 0.005*\"3\"')\n",
      "(4, u'0.022*\"1\" + 0.015*\"2\" + 0.015*\"x\" + 0.010*\"algorithm\" + 0.010*\"k\" + 0.009*\"n\" + 0.009*\"0\" + 0.007*\"f\" + 0.007*\"function\" + 0.006*\"p\"')\n",
      "(5, u'0.016*\"1\" + 0.011*\"0\" + 0.009*\"2\" + 0.009*\"x\" + 0.009*\"n\" + 0.007*\"p\" + 0.007*\"model\" + 0.007*\"j\" + 0.006*\"c\" + 0.006*\"r\"')\n",
      "(6, u'0.012*\"model\" + 0.011*\"1\" + 0.008*\"0\" + 0.008*\"2\" + 0.007*\"data\" + 0.005*\"x\" + 0.005*\"k\" + 0.005*\"p\" + 0.005*\"b\" + 0.004*\"j\"')\n",
      "(7, u'0.010*\"policy\" + 0.009*\"1\" + 0.009*\"learning\" + 0.008*\"state\" + 0.006*\"algorithm\" + 0.006*\"2\" + 0.006*\"action\" + 0.006*\"reward\" + 0.006*\"s\" + 0.005*\"a\"')\n",
      "(8, u'0.010*\"2\" + 0.009*\"1\" + 0.007*\"data\" + 0.006*\"set\" + 0.006*\"learning\" + 0.005*\"method\" + 0.005*\"kernel\" + 0.005*\"training\" + 0.004*\"feature\" + 0.004*\"algorithm\"')\n",
      "(9, u'0.009*\"network\" + 0.008*\"1\" + 0.007*\"model\" + 0.006*\"2\" + 0.005*\"learning\" + 0.004*\"object\" + 0.004*\"3\" + 0.004*\"x\" + 0.004*\"training\" + 0.004*\"j\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 2),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 2),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 2),\n",
       " (23, 1),\n",
       " (24, 4),\n",
       " (25, 3),\n",
       " (26, 6),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 2),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 2),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 2),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 2),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 2),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 2),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 2),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 2),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 1),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 3),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 2),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 2),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 2),\n",
       " (230, 1),\n",
       " (231, 2),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 2),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 13),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 2),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 3),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 3),\n",
       " (272, 1),\n",
       " (273, 4),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 2),\n",
       " (279, 2),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 2),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 8),\n",
       " (299, 1),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 1),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 6),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 1),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (335, 1),\n",
       " (336, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 1),\n",
       " (342, 3),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 1),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 1),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 1),\n",
       " (374, 1),\n",
       " (375, 1),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 1),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 2),\n",
       " (390, 1),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (397, 1),\n",
       " (398, 1),\n",
       " (399, 1),\n",
       " (400, 3),\n",
       " (401, 1),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 1),\n",
       " (410, 1),\n",
       " (411, 3),\n",
       " (412, 2),\n",
       " (413, 1),\n",
       " (414, 1),\n",
       " (415, 1),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 3),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 1),\n",
       " (435, 1),\n",
       " (436, 1),\n",
       " (437, 1),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 2),\n",
       " (441, 1),\n",
       " (442, 1),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 1),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 1),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 1),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 1),\n",
       " (458, 1),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 1),\n",
       " (462, 1),\n",
       " (463, 1),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (468, 1),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 1),\n",
       " (472, 2),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 1),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 1),\n",
       " (482, 1),\n",
       " (483, 3),\n",
       " (484, 3),\n",
       " (485, 1),\n",
       " (486, 1),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 3),\n",
       " (490, 1),\n",
       " (491, 1),\n",
       " (492, 1),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 2),\n",
       " (506, 1),\n",
       " (507, 1),\n",
       " (508, 1),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 1),\n",
       " (512, 1),\n",
       " (513, 1),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 1),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 2),\n",
       " (531, 1),\n",
       " (532, 2),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 1),\n",
       " (538, 1),\n",
       " (539, 1),\n",
       " (540, 1),\n",
       " (541, 2),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 1),\n",
       " (548, 1),\n",
       " (549, 8),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 1),\n",
       " (555, 2),\n",
       " (556, 1),\n",
       " (557, 1),\n",
       " (558, 1),\n",
       " (559, 9),\n",
       " (560, 1),\n",
       " (561, 1),\n",
       " (562, 1),\n",
       " (563, 1),\n",
       " (564, 1),\n",
       " (565, 2),\n",
       " (566, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 2),\n",
       " (570, 1),\n",
       " (571, 1),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 1),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 1),\n",
       " (581, 1),\n",
       " (582, 1),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 2),\n",
       " (586, 1),\n",
       " (587, 1),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 1),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (594, 1),\n",
       " (595, 1),\n",
       " (596, 1),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 1),\n",
       " (600, 1),\n",
       " (601, 1),\n",
       " (602, 1),\n",
       " (603, 1),\n",
       " (604, 1),\n",
       " (605, 1),\n",
       " (606, 1),\n",
       " (607, 6),\n",
       " (608, 1),\n",
       " (609, 4),\n",
       " (610, 1),\n",
       " (611, 1),\n",
       " (612, 1),\n",
       " (613, 1),\n",
       " (614, 2),\n",
       " (615, 1),\n",
       " (616, 3),\n",
       " (617, 2),\n",
       " (618, 2),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 1),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 1),\n",
       " (626, 1),\n",
       " (627, 1),\n",
       " (628, 1),\n",
       " (629, 1),\n",
       " (630, 1),\n",
       " (631, 1),\n",
       " (632, 1),\n",
       " (633, 1),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 1),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 1),\n",
       " (640, 1),\n",
       " (641, 2),\n",
       " (642, 1),\n",
       " (643, 1),\n",
       " (644, 1),\n",
       " (645, 1),\n",
       " (646, 2),\n",
       " (647, 1),\n",
       " (648, 2),\n",
       " (649, 1),\n",
       " (650, 2),\n",
       " (651, 1),\n",
       " (652, 1),\n",
       " (653, 1),\n",
       " (654, 1),\n",
       " (655, 1),\n",
       " (656, 1),\n",
       " (657, 1),\n",
       " (658, 1),\n",
       " (659, 1),\n",
       " (660, 1),\n",
       " (661, 1),\n",
       " (662, 1),\n",
       " (663, 1),\n",
       " (664, 1),\n",
       " (665, 1),\n",
       " (666, 1),\n",
       " (667, 1),\n",
       " (668, 1),\n",
       " (669, 1),\n",
       " (670, 1),\n",
       " (671, 1),\n",
       " (672, 1),\n",
       " (673, 1),\n",
       " (674, 1),\n",
       " (675, 1),\n",
       " (676, 1),\n",
       " (677, 1),\n",
       " (678, 1),\n",
       " (679, 1),\n",
       " (680, 1),\n",
       " (681, 1),\n",
       " (682, 1),\n",
       " (683, 4),\n",
       " (684, 2),\n",
       " (685, 4),\n",
       " (686, 2),\n",
       " (687, 5),\n",
       " (688, 2),\n",
       " (689, 2),\n",
       " (690, 1),\n",
       " (691, 1),\n",
       " (692, 1),\n",
       " (693, 1),\n",
       " (694, 2),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (697, 1),\n",
       " (698, 2),\n",
       " (699, 1),\n",
       " (700, 1),\n",
       " (701, 1),\n",
       " (702, 1),\n",
       " (703, 1),\n",
       " (704, 1),\n",
       " (705, 1),\n",
       " (706, 1),\n",
       " (707, 1),\n",
       " (708, 1),\n",
       " (709, 1),\n",
       " (710, 1),\n",
       " (711, 1),\n",
       " (712, 1),\n",
       " (713, 1),\n",
       " (714, 1),\n",
       " (715, 1),\n",
       " (716, 1),\n",
       " (717, 1),\n",
       " (718, 2),\n",
       " (719, 1),\n",
       " (720, 1),\n",
       " (721, 1),\n",
       " (722, 1),\n",
       " (723, 1),\n",
       " (724, 1),\n",
       " (725, 1),\n",
       " (726, 1),\n",
       " (727, 1),\n",
       " (728, 1),\n",
       " (729, 1),\n",
       " (730, 1),\n",
       " (731, 1),\n",
       " (732, 2),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 1),\n",
       " (736, 1),\n",
       " (737, 1),\n",
       " (738, 1),\n",
       " (739, 1),\n",
       " (740, 1),\n",
       " (741, 1),\n",
       " (742, 2),\n",
       " (743, 1),\n",
       " (744, 1),\n",
       " (745, 1),\n",
       " (746, 1),\n",
       " (747, 1),\n",
       " (748, 1),\n",
       " (749, 1),\n",
       " (750, 2),\n",
       " (751, 1),\n",
       " (752, 1),\n",
       " (753, 1),\n",
       " (754, 1),\n",
       " (755, 1),\n",
       " (756, 1),\n",
       " (757, 2),\n",
       " (758, 1),\n",
       " (759, 1),\n",
       " (760, 1),\n",
       " (761, 1),\n",
       " (762, 1),\n",
       " (763, 1),\n",
       " (764, 1),\n",
       " (765, 1),\n",
       " (766, 1),\n",
       " (767, 2),\n",
       " (768, 1),\n",
       " (769, 1),\n",
       " (770, 1),\n",
       " (771, 1),\n",
       " (772, 1),\n",
       " (773, 2),\n",
       " (774, 1),\n",
       " (775, 1),\n",
       " (776, 1),\n",
       " (777, 1),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (780, 1),\n",
       " (781, 2),\n",
       " (782, 1),\n",
       " (783, 1),\n",
       " (784, 1),\n",
       " (785, 1),\n",
       " (786, 1),\n",
       " (787, 1),\n",
       " (788, 1),\n",
       " (789, 1),\n",
       " (790, 1),\n",
       " (791, 1),\n",
       " (792, 1),\n",
       " (793, 1),\n",
       " (794, 1),\n",
       " (795, 1),\n",
       " (796, 1),\n",
       " (797, 1),\n",
       " (798, 1),\n",
       " (799, 1),\n",
       " (800, 1),\n",
       " (801, 1),\n",
       " (802, 1),\n",
       " (803, 1),\n",
       " (804, 3),\n",
       " (805, 1),\n",
       " (806, 1),\n",
       " (807, 1),\n",
       " (808, 1),\n",
       " (809, 1),\n",
       " (810, 1),\n",
       " (811, 2),\n",
       " (812, 1),\n",
       " (813, 1),\n",
       " (814, 1),\n",
       " (815, 1),\n",
       " (816, 1),\n",
       " (817, 1),\n",
       " (818, 1),\n",
       " (819, 1),\n",
       " (820, 1),\n",
       " (821, 3),\n",
       " (822, 1),\n",
       " (823, 1),\n",
       " (824, 1),\n",
       " (825, 1),\n",
       " (826, 1),\n",
       " (827, 1),\n",
       " (828, 1),\n",
       " (829, 1),\n",
       " (830, 1),\n",
       " (831, 1),\n",
       " (832, 1),\n",
       " (833, 1),\n",
       " (834, 1),\n",
       " (835, 1),\n",
       " (836, 1),\n",
       " (837, 1),\n",
       " (838, 1),\n",
       " (839, 1),\n",
       " (840, 1),\n",
       " (841, 1),\n",
       " (842, 1),\n",
       " (843, 1),\n",
       " (844, 2),\n",
       " (845, 1),\n",
       " (846, 1),\n",
       " (847, 1),\n",
       " (848, 1),\n",
       " (849, 1),\n",
       " (850, 1),\n",
       " (851, 6),\n",
       " (852, 1),\n",
       " (853, 1),\n",
       " (854, 1),\n",
       " (855, 1),\n",
       " (856, 1),\n",
       " (857, 1),\n",
       " (858, 1),\n",
       " (859, 1),\n",
       " (860, 1),\n",
       " (861, 1),\n",
       " (862, 1),\n",
       " (863, 1),\n",
       " (864, 1),\n",
       " (865, 1),\n",
       " (866, 1),\n",
       " (867, 1),\n",
       " (868, 1),\n",
       " (869, 1),\n",
       " (870, 1),\n",
       " (871, 1),\n",
       " (872, 1),\n",
       " (873, 1),\n",
       " (874, 1),\n",
       " (875, 1),\n",
       " (876, 1),\n",
       " (877, 1),\n",
       " (878, 1),\n",
       " (879, 1),\n",
       " (880, 2),\n",
       " (881, 1),\n",
       " (882, 1),\n",
       " (883, 1),\n",
       " (884, 3),\n",
       " (885, 1),\n",
       " (886, 1),\n",
       " (887, 1),\n",
       " (888, 1),\n",
       " (889, 1),\n",
       " (890, 1),\n",
       " (891, 1),\n",
       " (892, 1),\n",
       " (893, 3),\n",
       " (894, 1),\n",
       " (895, 1),\n",
       " (896, 1),\n",
       " (897, 1),\n",
       " (898, 1),\n",
       " (899, 1),\n",
       " (900, 1),\n",
       " (901, 1),\n",
       " (902, 1),\n",
       " (903, 1),\n",
       " (904, 1),\n",
       " (905, 1),\n",
       " (906, 1),\n",
       " (907, 1),\n",
       " (908, 1),\n",
       " (909, 1),\n",
       " (910, 1),\n",
       " (911, 2),\n",
       " (912, 1),\n",
       " (913, 2),\n",
       " (914, 1),\n",
       " (915, 1),\n",
       " (916, 1),\n",
       " (917, 3),\n",
       " (918, 1),\n",
       " (919, 1),\n",
       " (920, 1),\n",
       " (921, 1),\n",
       " (922, 1),\n",
       " (923, 1),\n",
       " (924, 1),\n",
       " (925, 1),\n",
       " (926, 1),\n",
       " (927, 1),\n",
       " (928, 1),\n",
       " (929, 1),\n",
       " (930, 1),\n",
       " (931, 2),\n",
       " (932, 1),\n",
       " (933, 1),\n",
       " (934, 1),\n",
       " (935, 1),\n",
       " (936, 1),\n",
       " (937, 1),\n",
       " (938, 1),\n",
       " (939, 1),\n",
       " (940, 1),\n",
       " (941, 1),\n",
       " (942, 2),\n",
       " (943, 1),\n",
       " (944, 1),\n",
       " (945, 1),\n",
       " (946, 1),\n",
       " (947, 1),\n",
       " (948, 1),\n",
       " (949, 1),\n",
       " (950, 1),\n",
       " (951, 1),\n",
       " (952, 1),\n",
       " (953, 1),\n",
       " (954, 9),\n",
       " (955, 1),\n",
       " (956, 1),\n",
       " (957, 1),\n",
       " (958, 1),\n",
       " (959, 1),\n",
       " (960, 1),\n",
       " (961, 1),\n",
       " (962, 1),\n",
       " (963, 1),\n",
       " (964, 1),\n",
       " (965, 1),\n",
       " (966, 1),\n",
       " (967, 1),\n",
       " (968, 1),\n",
       " (969, 2),\n",
       " (970, 1),\n",
       " (971, 1),\n",
       " (972, 1),\n",
       " (973, 1),\n",
       " (974, 2),\n",
       " (975, 1),\n",
       " (976, 1),\n",
       " (977, 1),\n",
       " (978, 2),\n",
       " (979, 1),\n",
       " (980, 1),\n",
       " (981, 1),\n",
       " (982, 1),\n",
       " (983, 1),\n",
       " (984, 1),\n",
       " (985, 1),\n",
       " (986, 3),\n",
       " (987, 1),\n",
       " (988, 1),\n",
       " (989, 1),\n",
       " (990, 1),\n",
       " (991, 1),\n",
       " (992, 1),\n",
       " (993, 1),\n",
       " (994, 2),\n",
       " (995, 1),\n",
       " (996, 1),\n",
       " (997, 1),\n",
       " (998, 1),\n",
       " (999, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.001*\"reinforcement_learning\" + 0.001*\"et_al\" + 0.001*\"s_a\" + 0.001*\"1_2\" + 0.000*\"machine_learning\" + 0.000*\"value_function\" + 0.000*\"reward_function\" + 0.000*\"0_1\" + 0.000*\"0_0\" + 0.000*\"neural_network\"')\n",
      "(1, u'0.001*\"1_1\" + 0.001*\"et_al\" + 0.001*\"0_1\" + 0.000*\"1_2\" + 0.000*\"machine_learning\" + 0.000*\"x_x\" + 0.000*\"figure_1\" + 0.000*\"2_2\" + 0.000*\"k_k\" + 0.000*\"figure_2\"')\n",
      "(2, u'0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"x_x\" + 0.001*\"neural_information\" + 0.001*\"2_2\" + 0.001*\"0_1\" + 0.001*\"information_processing\"')\n",
      "(3, u'0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_2\" + 0.001*\"x_x\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"processing_system\" + 0.001*\"0_0\" + 0.001*\"neural_network\" + 0.000*\"figure_1\"')\n",
      "(4, u'0.001*\"et_al\" + 0.001*\"x_x\" + 0.001*\"1_2\" + 0.001*\"0_0\" + 0.001*\"information_processing\" + 0.001*\"processing_system\" + 0.001*\"0_1\" + 0.000*\"machine_learning\" + 0.000*\"neural_network\" + 0.000*\"gaussian_process\"')\n",
      "(5, u'0.001*\"et_al\" + 0.001*\"neural_network\" + 0.000*\"figure_2\" + 0.000*\"computer_vision\" + 0.000*\"arxiv_preprint\" + 0.000*\"1_2\" + 0.000*\"information_processing\" + 0.000*\"figure_3\" + 0.000*\"figure_1\" + 0.000*\"ground_truth\"')\n",
      "(6, u'0.003*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"hidden_unit\" + 0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"figure_1\" + 0.001*\"information_processing\" + 0.001*\"figure_2\" + 0.000*\"figure_3\"')\n",
      "(7, u'0.002*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"neural_information\" + 0.001*\"figure_1\" + 0.000*\"1_2\" + 0.000*\"0_0\" + 0.000*\"information_processing\" + 0.000*\"figure_2\" + 0.000*\"table_1\" + 0.000*\"training_set\"')\n",
      "(8, u'0.001*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"0_0\" + 0.001*\"1_2\" + 0.001*\"training_set\" + 0.000*\"table_1\" + 0.000*\"x_x\"')\n",
      "(9, u'0.001*\"f_x\" + 0.001*\"1_2\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"2_2\" + 0.001*\"x_x\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"lower_bound\" + 0.001*\"loss_function\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# ldamodel.save('nips.bigrams')\n",
    "\n",
    "#Load model\n",
    "ldamodel = Lda.load('nips.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.001*\"reinforcement_learning\" + 0.001*\"et_al\" + 0.001*\"s_a\" + 0.001*\"1_2\" + 0.000*\"machine_learning\"')\n",
      "(1, u'0.001*\"1_1\" + 0.001*\"et_al\" + 0.001*\"0_1\" + 0.000*\"1_2\" + 0.000*\"machine_learning\"')\n",
      "(2, u'0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"0_0\"')\n",
      "(3, u'0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_2\" + 0.001*\"x_x\" + 0.001*\"1_1\"')\n",
      "(4, u'0.001*\"et_al\" + 0.001*\"x_x\" + 0.001*\"1_2\" + 0.001*\"0_0\" + 0.001*\"information_processing\"')\n",
      "(5, u'0.001*\"et_al\" + 0.001*\"neural_network\" + 0.000*\"figure_2\" + 0.000*\"computer_vision\" + 0.000*\"arxiv_preprint\"')\n",
      "(6, u'0.003*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"hidden_unit\" + 0.001*\"1_1\"')\n",
      "(7, u'0.002*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"neural_information\" + 0.001*\"figure_1\" + 0.000*\"1_2\"')\n",
      "(8, u'0.001*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_1\" + 0.001*\"0_1\"')\n",
      "(9, u'0.001*\"f_x\" + 0.001*\"1_2\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"2_2\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=10, num_words=5):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/mrisdal/fake-news/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11512    Here's something interesting from The Unz Revi...\n",
       "4868      Doomsday Election\\nBy Mike Whitney November 0...\n",
       "3093     \\nA new Wikileaks email just exposed that Hill...\n",
       "9415     \\n“Go ahead. Look at them,” said Hillary Clint...\n",
       "12030    « L’art de la guerre »\\nComment voter « Non » ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('fake.csv', usecols = ['text'])\n",
    "ds.dropna(axis=0, inplace=True, subset=['text'])\n",
    "ds = ds.sample(frac=1.0)\n",
    "ds['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12953"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word.decode('utf-8')) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in ds['text']]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'here',\n",
       " u'something',\n",
       " u'interesting',\n",
       " u'unz',\n",
       " u'review',\n",
       " u'recipient',\n",
       " u'name',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'read',\n",
       " u'book',\n",
       " u'wrote',\n",
       " u'them',\n",
       " u'credit',\n",
       " u'vdarecom',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'hadn\\u2019t',\n",
       " u'crazy',\n",
       " u'thinskinned',\n",
       " u'moral',\n",
       " u'lout',\n",
       " u'hold',\n",
       " u'constitution',\n",
       " u'contempt',\n",
       " u'could',\n",
       " u'supported',\n",
       " u'him',\n",
       " u'maybe',\n",
       " u'mushy',\n",
       " u'moderate',\n",
       " u'think',\n",
       " u'alike',\n",
       " u'rod',\n",
       " u'dreher',\n",
       " u'tragedy',\n",
       " u'trump',\n",
       " u'american',\n",
       " u'conservative',\n",
       " u'october',\n",
       " u'24',\n",
       " u'2016',\n",
       " u'praising',\n",
       " u'new',\n",
       " u'york',\n",
       " u'time',\n",
       " u'token',\n",
       " u'conservative',\n",
       " u'ross',\n",
       " u'douthat',\n",
       " u'saying',\n",
       " u'danger',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'presidency',\n",
       " u'familiar',\n",
       " u'trump\\u2019s',\n",
       " u'authoritarian',\n",
       " u'unknown',\n",
       " u'live',\n",
       " u'politics',\n",
       " u'already',\n",
       " u'they\\u2019re',\n",
       " u'danger',\n",
       " u'elite',\n",
       " u'groupthink',\n",
       " u'beltway',\n",
       " u'power',\n",
       " u'worship',\n",
       " u'cult',\n",
       " u'presidential',\n",
       " u'action',\n",
       " u'service',\n",
       " u'dubious',\n",
       " u'ideal',\n",
       " u'they\\u2019re',\n",
       " u'danger',\n",
       " u'recklessness',\n",
       " u'radicalism',\n",
       " u'doesn\\u2019t',\n",
       " u'recognize',\n",
       " u'either',\n",
       " u'it\\u2019s',\n",
       " u'convinced',\n",
       " u'idea',\n",
       " u'mainstream',\n",
       " u'commonplace',\n",
       " u'among',\n",
       " u'great',\n",
       " u'good',\n",
       " u'cannot',\n",
       " u'possibly',\n",
       " u'folly',\n",
       " u'danger',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'october',\n",
       " u'22',\n",
       " u'2016',\n",
       " u'dreher\\u2019s',\n",
       " u'emphasis',\n",
       " u'dreher',\n",
       " u'comment',\n",
       " u'boy',\n",
       " u'ever',\n",
       " u'true',\n",
       " u'read',\n",
       " u'whole',\n",
       " u'thing',\n",
       " u'along',\n",
       " u'line',\n",
       " u'quite',\n",
       " u'good',\n",
       " u'peggy',\n",
       " u'noonan',\n",
       " u'column',\n",
       " u'wsj',\n",
       " u'last',\n",
       " u'week',\n",
       " u'imagine',\n",
       " u'sane',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'october',\n",
       " u'20',\n",
       " u'2016now',\n",
       " u'ala',\n",
       " u'behind',\n",
       " u'paywall',\n",
       " u'found',\n",
       " u'whole',\n",
       " u'thing',\n",
       " u'saying',\n",
       " u'trump',\n",
       " u'\\u201cnut\\u201d',\n",
       " u'\\u2014',\n",
       " u'clearly',\n",
       " u'\\u2014',\n",
       " u'would',\n",
       " u'winning',\n",
       " u'thing',\n",
       " u'landslide',\n",
       " u'lot',\n",
       " u'folk',\n",
       " u'sick',\n",
       " u'tired',\n",
       " u'status',\n",
       " u'quo',\n",
       " u'hillary',\n",
       " u'represents',\n",
       " u'wait',\n",
       " u'minute',\n",
       " u'find',\n",
       " u'mind',\n",
       " u'circling',\n",
       " u'back',\n",
       " u'patrick',\n",
       " u'j',\n",
       " u'buchanan',\n",
       " u'devout',\n",
       " u'christian',\n",
       " u'longtime',\n",
       " u'conservative',\n",
       " u'loyal',\n",
       " u'republican',\n",
       " u'endorsed',\n",
       " u'bob',\n",
       " u'dole',\n",
       " u'1996',\n",
       " u'george',\n",
       " u'w',\n",
       " u'bush',\n",
       " u'2004',\n",
       " u'buchanan',\n",
       " u'truly',\n",
       " u'creative',\n",
       " u'political',\n",
       " u'thinker',\n",
       " u'wrote',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'playbook',\n",
       " u'immigration',\n",
       " u'freetrade',\n",
       " u'foreign',\n",
       " u'policy',\n",
       " u'\\u2014and',\n",
       " u'excommunicated',\n",
       " u'cult',\n",
       " u'trueconservatism',\n",
       " u'opposing',\n",
       " u'disastrous',\n",
       " u'iraq',\n",
       " u'war',\n",
       " u'then',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'different',\n",
       " u'messenger',\n",
       " u'one',\n",
       " u'carried',\n",
       " u'buchanan\\u2019s',\n",
       " u'torch',\n",
       " u'foreign',\n",
       " u'policy',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'wasn\\u2019t',\n",
       " u'strong',\n",
       " u'buchanan',\n",
       " u'immigration',\n",
       " u'freetrade',\n",
       " u'mr',\n",
       " u'constitution',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'like',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'carried',\n",
       " u'none',\n",
       " u'trump\\u2019s',\n",
       " u'character',\n",
       " u'baggage',\n",
       " u'woman',\n",
       " u'morality',\n",
       " u'religion',\n",
       " u'strong',\n",
       " u'constitution',\n",
       " u'bornagain',\n",
       " u'constitutional',\n",
       " u'conservative',\n",
       " u'respond',\n",
       " u'ron',\n",
       " u'paul\\u2019s',\n",
       " u'candidacy',\n",
       " u'2012',\n",
       " u'republican',\n",
       " u'convention',\n",
       " u'crushed',\n",
       " u'him',\n",
       " u'chaos',\n",
       " u'convention',\n",
       " u'floor',\n",
       " u'rnc',\n",
       " u'block',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'delegate',\n",
       " u'alters',\n",
       " u'seating',\n",
       " u'rule',\n",
       " u'democracynow',\n",
       " u'august',\n",
       " u'29',\n",
       " u'2012',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'too',\n",
       " u'different',\n",
       " u'messenger',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'huckabee\\u2019s',\n",
       " u'campaign',\n",
       " u'anticipated',\n",
       " u'trump\\u2019s',\n",
       " u'campaign',\n",
       " u'entitlement',\n",
       " u'wanted',\n",
       " u'bluecollar',\n",
       " u'populist',\n",
       " u'working',\n",
       " u'class',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'tilted',\n",
       " u'toward',\n",
       " u'main',\n",
       " u'street',\n",
       " u'k',\n",
       " u'street',\n",
       " u'chamber',\n",
       " u'commerce',\n",
       " u'also',\n",
       " u'like',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'shared',\n",
       " u'none',\n",
       " u'trump\\u2019s',\n",
       " u'character',\n",
       " u'baggage',\n",
       " u'woman',\n",
       " u'morality',\n",
       " u'religion',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'mr',\n",
       " u'evangelical',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'2016',\n",
       " u'campaign',\n",
       " u'huckabee',\n",
       " u'styled',\n",
       " u'\\u201cgod',\n",
       " u'gun',\n",
       " u'grit',\n",
       " u'gravy\\u201d',\n",
       " u'conservative',\n",
       " u'yet',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'caricatured',\n",
       " u'neonazi',\n",
       " u'antisemite',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'crazy',\n",
       " u'wild',\n",
       " u'eyed',\n",
       " u'kook',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'unacceptable',\n",
       " u'cornpone',\n",
       " u'welfarestater',\n",
       " u'unlike',\n",
       " u'john',\n",
       " u'mccain',\n",
       " u'mitt',\n",
       " u'romney',\n",
       " u'\\xa1jeb',\n",
       " u'non',\n",
       " u'e',\n",
       " u'\\u201csane',\n",
       " u'donald',\n",
       " u'trumps\\u201d',\n",
       " u'acceptable',\n",
       " u'classicalliberal',\n",
       " u'selfanointed',\n",
       " u'\\u201cgoverning',\n",
       " u'wing\\u201d',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'tac',\n",
       " u'\\u2019s',\n",
       " u'dreher',\n",
       " u'wimp',\n",
       " u'on',\n",
       " u'trumpers',\n",
       " u'likewise',\n",
       " u'strongly',\n",
       " u'tempted',\n",
       " u'indulge',\n",
       " u'bitter',\n",
       " u'\\u201cyou',\n",
       " u'stabbed',\n",
       " u'back\\u201d',\n",
       " u'polemic',\n",
       " u'work',\n",
       " u'advantage',\n",
       " u'president',\n",
       " u'h',\n",
       " u'clinton',\n",
       " u'course',\n",
       " u'needed',\n",
       " u'gop',\n",
       " u'establishment',\n",
       " u'humble',\n",
       " u'enough',\n",
       " u'admit',\n",
       " u'who',\n",
       " u'like',\n",
       " u'noonan',\n",
       " u'accept',\n",
       " u'critique',\n",
       " u'party',\n",
       " u'system',\n",
       " u'trump\\u2019s',\n",
       " u'candidacy',\n",
       " u'embodies',\n",
       " u'however',\n",
       " u'well',\n",
       " u'nuttily',\n",
       " u'trump',\n",
       " u'insurgent',\n",
       " u'\\u2014',\n",
       " u'including',\n",
       " u'leader',\n",
       " u'\\u2014',\n",
       " u'need',\n",
       " u'sense',\n",
       " u'realize',\n",
       " u'advantage',\n",
       " u'drag',\n",
       " u'fight',\n",
       " u'republican',\n",
       " u'past',\n",
       " u'election',\n",
       " u'candidate',\n",
       " u'received',\n",
       " u'thorough',\n",
       " u'resounding',\n",
       " u'rejection',\n",
       " u'voter',\n",
       " u'election',\n",
       " u'likely',\n",
       " u'would',\n",
       " u'consistently',\n",
       " u'spoken',\n",
       " u'acted',\n",
       " u'like',\n",
       " u'nut',\n",
       " u'think',\n",
       " u'humility',\n",
       " u'side',\n",
       " u'uniting',\n",
       " u'face',\n",
       " u'hillaryism',\n",
       " u'likely',\n",
       " u'happen',\n",
       " u'no',\n",
       " u'not',\n",
       " u'hope',\n",
       " u'i\\u2019m',\n",
       " u'wrong',\n",
       " u'trump',\n",
       " u'people',\n",
       " u'like',\n",
       " u'candidate',\n",
       " u'known',\n",
       " u'ability',\n",
       " u'think',\n",
       " u'strategically',\n",
       " u'restrain',\n",
       " u'good',\n",
       " u'bitterness',\n",
       " u'spite',\n",
       " u'among',\n",
       " u'republican',\n",
       " u'regular',\n",
       " u'going',\n",
       " u'blind',\n",
       " u'role',\n",
       " u'creating',\n",
       " u'mess',\n",
       " u'my',\n",
       " u'emphasis',\n",
       " u'isn\\u2019t',\n",
       " u'\\u201cpolemics\\u201d\\u2014it',\n",
       " u'historical',\n",
       " u'fact',\n",
       " u'major',\n",
       " u'party',\n",
       " u'candidate',\n",
       " u'american',\n",
       " u'history',\n",
       " u'including',\n",
       " u'william',\n",
       " u'howard',\n",
       " u'taft',\n",
       " u'1912',\n",
       " u'barry',\n",
       " u'goldwater',\n",
       " u'1964',\n",
       " u'stabbed',\n",
       " u'back',\n",
       " u'member',\n",
       " u'party',\n",
       " u'major',\n",
       " u'party',\n",
       " u'candidate',\n",
       " u'history',\n",
       " u'ever',\n",
       " u'taken',\n",
       " u'fire',\n",
       " u'\\u201cleading',\n",
       " u'intellectuals\\u201d',\n",
       " u'party',\n",
       " u'trump',\n",
       " u'fighting',\n",
       " u'twofront',\n",
       " u'war',\n",
       " u'since',\n",
       " u'may\\u2014the',\n",
       " u'war',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'left',\n",
       " u'war',\n",
       " u'nevertrumper',\n",
       " u'conservative',\n",
       " u'intellectual',\n",
       " u'hack',\n",
       " u'party',\n",
       " u'trump\\u2019s',\n",
       " u'struggle',\n",
       " u'convention',\n",
       " u'general',\n",
       " u'election',\n",
       " u'due',\n",
       " u'single',\n",
       " u'demographic',\n",
       " u'college',\n",
       " u'educated',\n",
       " u'suburban',\n",
       " u'white',\n",
       " u'republican',\n",
       " u'especially',\n",
       " u'woman',\n",
       " u'people',\n",
       " u'voted',\n",
       " u'mitt',\n",
       " u'romney',\n",
       " u'2012',\n",
       " u'voted',\n",
       " u'\\xa1jeb',\n",
       " u'rubio',\n",
       " u'kasich',\n",
       " u'republican',\n",
       " u'primary',\n",
       " u'25',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'refused',\n",
       " u'back',\n",
       " u'trump',\n",
       " u'panicked',\n",
       " u'time',\n",
       " u'time',\n",
       " u'whenever',\n",
       " u'new',\n",
       " u'story',\n",
       " u'roils',\n",
       " u'news',\n",
       " u'cycle',\n",
       " u'nevertrump',\n",
       " u'still',\n",
       " u'lot',\n",
       " u'influence',\n",
       " u'establishmentoriented',\n",
       " u'voter',\n",
       " u'strategy',\n",
       " u'throw',\n",
       " u'gasoline',\n",
       " u'every',\n",
       " u'small',\n",
       " u'fire',\n",
       " u'news',\n",
       " u'cycle',\n",
       " u'order',\n",
       " u'panic',\n",
       " u'voter',\n",
       " u'fleeing',\n",
       " u'trump',\n",
       " u'that\\u2019s',\n",
       " u'happened',\n",
       " u'judge',\n",
       " u'curiel',\n",
       " u'incident',\n",
       " u'khizr',\n",
       " u'khan',\n",
       " u'fiasco',\n",
       " u'recently',\n",
       " u'access',\n",
       " u'hollywood',\n",
       " u'tape',\n",
       " u'trump\\u2019s',\n",
       " u'\\u201cfellow',\n",
       " u'republicans\\u201d',\n",
       " u'come',\n",
       " u'slam',\n",
       " u'make',\n",
       " u'story',\n",
       " u'far',\n",
       " u'bigger',\n",
       " u'deal',\n",
       " u'dynamic',\n",
       " u'it\\u2019s',\n",
       " u'true',\n",
       " u'say',\n",
       " u'happened',\n",
       " u'october',\n",
       " u'8',\n",
       " u'2016',\n",
       " u'day',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'died',\n",
       " u'day',\n",
       " u'nevertrump',\n",
       " u'coup',\n",
       " u'which13',\n",
       " u'republican',\n",
       " u'senate',\n",
       " u'14',\n",
       " u'elected',\n",
       " u'republican',\n",
       " u'publicly',\n",
       " u'abandoned',\n",
       " u'him',\n",
       " u'historically',\n",
       " u'unprecedented',\n",
       " u'betrayal',\n",
       " u'\\u2013',\n",
       " u'never',\n",
       " u'american',\n",
       " u'history',\n",
       " u'even',\n",
       " u'tr\\u2019s',\n",
       " u'shortlived',\n",
       " u'bull',\n",
       " u'moose',\n",
       " u'party',\n",
       " u'1912',\n",
       " u'elected',\n",
       " u'member',\n",
       " u'one',\n",
       " u'party',\n",
       " u'deserted',\n",
       " u'nominee',\n",
       " u'utterly',\n",
       " u'predictable',\n",
       " u'result',\n",
       " u'nevertrump',\n",
       " u'coup',\n",
       " u'party',\n",
       " u'split',\n",
       " u'showed',\n",
       " u'poll',\n",
       " u'event',\n",
       " u'altered',\n",
       " u'whole',\n",
       " u'trajectory',\n",
       " u'tone',\n",
       " u'race',\n",
       " u'would',\n",
       " u'happened',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'beaten',\n",
       " u'dole',\n",
       " u'1996',\n",
       " u'romney',\n",
       " u'2012',\n",
       " u'believe',\n",
       " u'result',\n",
       " u'would',\n",
       " u'think',\n",
       " u'republican',\n",
       " u'establishment',\n",
       " u'would',\n",
       " u'joined',\n",
       " u'force',\n",
       " u'democratic',\n",
       " u'establishment',\n",
       " u'order',\n",
       " u'preserve',\n",
       " u'neoliberal',\n",
       " u'globalist',\n",
       " u'status',\n",
       " u'quo',\n",
       " u'would',\n",
       " u'kind',\n",
       " u'neverbuchanan',\n",
       " u'neverpaul',\n",
       " u'neverhuckabee',\n",
       " u'campaign',\n",
       " u'forerunner',\n",
       " u'trumpism',\n",
       " u'republican',\n",
       " u'nomination',\n",
       " u'would',\n",
       " u'similar',\n",
       " u'fight',\n",
       " u'\\u201csave',\n",
       " u'soul\\u201d',\n",
       " u'\\u201cconservatism\\u201d',\n",
       " u'establishment',\n",
       " u'candidate',\n",
       " u'loses',\n",
       " u'primary',\n",
       " u'\\u201csoul\\u201d',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'always',\n",
       " u'risk',\n",
       " u'trump',\n",
       " u'loses',\n",
       " u'landslide',\n",
       " u'response',\n",
       " u'gop',\n",
       " u'establishment',\n",
       " u'future',\n",
       " u'insurgent',\n",
       " u'candidate',\n",
       " u'precedent',\n",
       " u'set',\n",
       " u'2016',\n",
       " u'sabotage',\n",
       " u'nominee',\n",
       " u'order',\n",
       " u'elect',\n",
       " u'democrat',\n",
       " u'hang',\n",
       " u'power',\n",
       " u'this',\n",
       " u'impossible',\n",
       " u'reform',\n",
       " u'gop',\n",
       " u'primary',\n",
       " u'system',\n",
       " u'yes',\n",
       " u'must',\n",
       " u'face',\n",
       " u'fact',\n",
       " u'\\u201cdrag',\n",
       " u'fight',\n",
       " u'out\\u201d',\n",
       " u'beyond',\n",
       " u'november',\n",
       " u'conservatism',\n",
       " u'inc',\n",
       " u'thrown',\n",
       " u'gauntlet',\n",
       " u'either',\n",
       " u'support',\n",
       " u'\\xa1jeb',\n",
       " u'2020',\n",
       " u'rubio',\n",
       " u'2020',\n",
       " u'candidate',\n",
       " u'minority',\n",
       " u'neoliberal',\n",
       " u'globalist',\n",
       " u'\\u201cgoverning',\n",
       " u'wing\\u201d',\n",
       " u'gop\\u2014or',\n",
       " u'else',\n",
       " u'brad',\n",
       " u'griffin',\n",
       " u'editor',\n",
       " u'occidental',\n",
       " u'dissent',\n",
       " u'writes',\n",
       " u'pen',\n",
       " u'name',\n",
       " u'hunter',\n",
       " u'wallace',\n",
       " u'reprinted',\n",
       " u'vdarecom',\n",
       " u'permission',\n",
       " u'author',\n",
       " u'representative']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(word for d in doc_clean for word in d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213611"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\\u043f\\u043e\\u0435\\u0437\\u0434\\u043e\\u0432', 4),\n",
       " (u'seward', 4),\n",
       " (u'mcguire', 4),\n",
       " (u'guilty\\u2019', 4),\n",
       " (u'posttrump', 4),\n",
       " (u'theistic', 4),\n",
       " (u'\\u201dask', 4),\n",
       " (u'\\u0448\\u043a\\u043e\\u043b\\u0430\\u0445', 4),\n",
       " (u'approximated', 4),\n",
       " (u'bugging', 4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 50000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\\u0432\\u0430\\u043c', 27),\n",
       " (u'ersten', 27),\n",
       " (u'risked', 27),\n",
       " (u'law\\u2019s', 27),\n",
       " (u'lehman', 27),\n",
       " (u'transpired', 27),\n",
       " (u'22nd', 27),\n",
       " (u'scoundrel', 27),\n",
       " (u'\\u042f\\u043f\\u043e\\u043d\\u0438\\u0438', 27),\n",
       " (u'don\\xe2\\u20ac\\u2122t', 27)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 15000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_words = dict(top_k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_clean_freqs = [[w for w in doc if w in top_k_words] for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'here',\n",
       " u'something',\n",
       " u'interesting',\n",
       " u'unz',\n",
       " u'review',\n",
       " u'recipient',\n",
       " u'name',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'read',\n",
       " u'book',\n",
       " u'wrote',\n",
       " u'them',\n",
       " u'credit',\n",
       " u'vdarecom',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'hadn\\u2019t',\n",
       " u'crazy',\n",
       " u'moral',\n",
       " u'hold',\n",
       " u'constitution',\n",
       " u'contempt',\n",
       " u'could',\n",
       " u'supported',\n",
       " u'him',\n",
       " u'maybe',\n",
       " u'moderate',\n",
       " u'think',\n",
       " u'alike',\n",
       " u'rod',\n",
       " u'tragedy',\n",
       " u'trump',\n",
       " u'american',\n",
       " u'conservative',\n",
       " u'october',\n",
       " u'24',\n",
       " u'2016',\n",
       " u'praising',\n",
       " u'new',\n",
       " u'york',\n",
       " u'time',\n",
       " u'token',\n",
       " u'conservative',\n",
       " u'ross',\n",
       " u'saying',\n",
       " u'danger',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'presidency',\n",
       " u'familiar',\n",
       " u'trump\\u2019s',\n",
       " u'authoritarian',\n",
       " u'unknown',\n",
       " u'live',\n",
       " u'politics',\n",
       " u'already',\n",
       " u'they\\u2019re',\n",
       " u'danger',\n",
       " u'elite',\n",
       " u'beltway',\n",
       " u'power',\n",
       " u'worship',\n",
       " u'cult',\n",
       " u'presidential',\n",
       " u'action',\n",
       " u'service',\n",
       " u'dubious',\n",
       " u'ideal',\n",
       " u'they\\u2019re',\n",
       " u'danger',\n",
       " u'radicalism',\n",
       " u'doesn\\u2019t',\n",
       " u'recognize',\n",
       " u'either',\n",
       " u'it\\u2019s',\n",
       " u'convinced',\n",
       " u'idea',\n",
       " u'mainstream',\n",
       " u'commonplace',\n",
       " u'among',\n",
       " u'great',\n",
       " u'good',\n",
       " u'cannot',\n",
       " u'possibly',\n",
       " u'folly',\n",
       " u'danger',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'october',\n",
       " u'22',\n",
       " u'2016',\n",
       " u'emphasis',\n",
       " u'comment',\n",
       " u'boy',\n",
       " u'ever',\n",
       " u'true',\n",
       " u'read',\n",
       " u'whole',\n",
       " u'thing',\n",
       " u'along',\n",
       " u'line',\n",
       " u'quite',\n",
       " u'good',\n",
       " u'column',\n",
       " u'wsj',\n",
       " u'last',\n",
       " u'week',\n",
       " u'imagine',\n",
       " u'sane',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'october',\n",
       " u'20',\n",
       " u'ala',\n",
       " u'behind',\n",
       " u'found',\n",
       " u'whole',\n",
       " u'thing',\n",
       " u'saying',\n",
       " u'trump',\n",
       " u'\\u2014',\n",
       " u'clearly',\n",
       " u'\\u2014',\n",
       " u'would',\n",
       " u'winning',\n",
       " u'thing',\n",
       " u'landslide',\n",
       " u'lot',\n",
       " u'folk',\n",
       " u'sick',\n",
       " u'tired',\n",
       " u'status',\n",
       " u'quo',\n",
       " u'hillary',\n",
       " u'represents',\n",
       " u'wait',\n",
       " u'minute',\n",
       " u'find',\n",
       " u'mind',\n",
       " u'circling',\n",
       " u'back',\n",
       " u'patrick',\n",
       " u'j',\n",
       " u'buchanan',\n",
       " u'christian',\n",
       " u'longtime',\n",
       " u'conservative',\n",
       " u'loyal',\n",
       " u'republican',\n",
       " u'endorsed',\n",
       " u'bob',\n",
       " u'dole',\n",
       " u'1996',\n",
       " u'george',\n",
       " u'w',\n",
       " u'bush',\n",
       " u'2004',\n",
       " u'buchanan',\n",
       " u'truly',\n",
       " u'creative',\n",
       " u'political',\n",
       " u'thinker',\n",
       " u'wrote',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'playbook',\n",
       " u'immigration',\n",
       " u'foreign',\n",
       " u'policy',\n",
       " u'cult',\n",
       " u'opposing',\n",
       " u'disastrous',\n",
       " u'iraq',\n",
       " u'war',\n",
       " u'then',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'different',\n",
       " u'messenger',\n",
       " u'one',\n",
       " u'carried',\n",
       " u'torch',\n",
       " u'foreign',\n",
       " u'policy',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'wasn\\u2019t',\n",
       " u'strong',\n",
       " u'buchanan',\n",
       " u'immigration',\n",
       " u'mr',\n",
       " u'constitution',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'like',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'carried',\n",
       " u'none',\n",
       " u'trump\\u2019s',\n",
       " u'character',\n",
       " u'baggage',\n",
       " u'woman',\n",
       " u'morality',\n",
       " u'religion',\n",
       " u'strong',\n",
       " u'constitution',\n",
       " u'constitutional',\n",
       " u'conservative',\n",
       " u'respond',\n",
       " u'ron',\n",
       " u'paul\\u2019s',\n",
       " u'candidacy',\n",
       " u'2012',\n",
       " u'republican',\n",
       " u'convention',\n",
       " u'crushed',\n",
       " u'him',\n",
       " u'chaos',\n",
       " u'convention',\n",
       " u'floor',\n",
       " u'rnc',\n",
       " u'block',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'delegate',\n",
       " u'rule',\n",
       " u'august',\n",
       " u'29',\n",
       " u'2012',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'too',\n",
       " u'different',\n",
       " u'messenger',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'campaign',\n",
       " u'anticipated',\n",
       " u'trump\\u2019s',\n",
       " u'campaign',\n",
       " u'entitlement',\n",
       " u'wanted',\n",
       " u'bluecollar',\n",
       " u'populist',\n",
       " u'working',\n",
       " u'class',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'toward',\n",
       " u'main',\n",
       " u'street',\n",
       " u'k',\n",
       " u'street',\n",
       " u'chamber',\n",
       " u'commerce',\n",
       " u'also',\n",
       " u'like',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'shared',\n",
       " u'none',\n",
       " u'trump\\u2019s',\n",
       " u'character',\n",
       " u'baggage',\n",
       " u'woman',\n",
       " u'morality',\n",
       " u'religion',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'mr',\n",
       " u'evangelical',\n",
       " u'2008',\n",
       " u'2012',\n",
       " u'2016',\n",
       " u'campaign',\n",
       " u'huckabee',\n",
       " u'\\u201cgod',\n",
       " u'gun',\n",
       " u'conservative',\n",
       " u'yet',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'neonazi',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'crazy',\n",
       " u'wild',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'unacceptable',\n",
       " u'unlike',\n",
       " u'john',\n",
       " u'mccain',\n",
       " u'mitt',\n",
       " u'romney',\n",
       " u'non',\n",
       " u'e',\n",
       " u'donald',\n",
       " u'acceptable',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'\\u2019s',\n",
       " u'on',\n",
       " u'likewise',\n",
       " u'strongly',\n",
       " u'tempted',\n",
       " u'indulge',\n",
       " u'bitter',\n",
       " u'\\u201cyou',\n",
       " u'stabbed',\n",
       " u'back\\u201d',\n",
       " u'work',\n",
       " u'advantage',\n",
       " u'president',\n",
       " u'h',\n",
       " u'clinton',\n",
       " u'course',\n",
       " u'needed',\n",
       " u'gop',\n",
       " u'establishment',\n",
       " u'humble',\n",
       " u'enough',\n",
       " u'admit',\n",
       " u'who',\n",
       " u'like',\n",
       " u'accept',\n",
       " u'critique',\n",
       " u'party',\n",
       " u'system',\n",
       " u'trump\\u2019s',\n",
       " u'candidacy',\n",
       " u'however',\n",
       " u'well',\n",
       " u'trump',\n",
       " u'insurgent',\n",
       " u'\\u2014',\n",
       " u'including',\n",
       " u'leader',\n",
       " u'\\u2014',\n",
       " u'need',\n",
       " u'sense',\n",
       " u'realize',\n",
       " u'advantage',\n",
       " u'drag',\n",
       " u'fight',\n",
       " u'republican',\n",
       " u'past',\n",
       " u'election',\n",
       " u'candidate',\n",
       " u'received',\n",
       " u'thorough',\n",
       " u'resounding',\n",
       " u'rejection',\n",
       " u'voter',\n",
       " u'election',\n",
       " u'likely',\n",
       " u'would',\n",
       " u'consistently',\n",
       " u'spoken',\n",
       " u'acted',\n",
       " u'like',\n",
       " u'nut',\n",
       " u'think',\n",
       " u'humility',\n",
       " u'side',\n",
       " u'uniting',\n",
       " u'face',\n",
       " u'likely',\n",
       " u'happen',\n",
       " u'no',\n",
       " u'not',\n",
       " u'hope',\n",
       " u'i\\u2019m',\n",
       " u'wrong',\n",
       " u'trump',\n",
       " u'people',\n",
       " u'like',\n",
       " u'candidate',\n",
       " u'known',\n",
       " u'ability',\n",
       " u'think',\n",
       " u'strategically',\n",
       " u'restrain',\n",
       " u'good',\n",
       " u'spite',\n",
       " u'among',\n",
       " u'republican',\n",
       " u'regular',\n",
       " u'going',\n",
       " u'blind',\n",
       " u'role',\n",
       " u'creating',\n",
       " u'mess',\n",
       " u'my',\n",
       " u'emphasis',\n",
       " u'isn\\u2019t',\n",
       " u'historical',\n",
       " u'fact',\n",
       " u'major',\n",
       " u'party',\n",
       " u'candidate',\n",
       " u'american',\n",
       " u'history',\n",
       " u'including',\n",
       " u'william',\n",
       " u'howard',\n",
       " u'barry',\n",
       " u'goldwater',\n",
       " u'1964',\n",
       " u'stabbed',\n",
       " u'back',\n",
       " u'member',\n",
       " u'party',\n",
       " u'major',\n",
       " u'party',\n",
       " u'candidate',\n",
       " u'history',\n",
       " u'ever',\n",
       " u'taken',\n",
       " u'fire',\n",
       " u'party',\n",
       " u'trump',\n",
       " u'fighting',\n",
       " u'war',\n",
       " u'since',\n",
       " u'war',\n",
       " u'hillary',\n",
       " u'clinton',\n",
       " u'left',\n",
       " u'war',\n",
       " u'conservative',\n",
       " u'intellectual',\n",
       " u'hack',\n",
       " u'party',\n",
       " u'trump\\u2019s',\n",
       " u'struggle',\n",
       " u'convention',\n",
       " u'general',\n",
       " u'election',\n",
       " u'due',\n",
       " u'single',\n",
       " u'demographic',\n",
       " u'college',\n",
       " u'educated',\n",
       " u'suburban',\n",
       " u'white',\n",
       " u'republican',\n",
       " u'especially',\n",
       " u'woman',\n",
       " u'people',\n",
       " u'voted',\n",
       " u'mitt',\n",
       " u'romney',\n",
       " u'2012',\n",
       " u'voted',\n",
       " u'rubio',\n",
       " u'kasich',\n",
       " u'republican',\n",
       " u'primary',\n",
       " u'25',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'refused',\n",
       " u'back',\n",
       " u'trump',\n",
       " u'panicked',\n",
       " u'time',\n",
       " u'time',\n",
       " u'whenever',\n",
       " u'new',\n",
       " u'story',\n",
       " u'news',\n",
       " u'cycle',\n",
       " u'nevertrump',\n",
       " u'still',\n",
       " u'lot',\n",
       " u'influence',\n",
       " u'voter',\n",
       " u'strategy',\n",
       " u'throw',\n",
       " u'gasoline',\n",
       " u'every',\n",
       " u'small',\n",
       " u'fire',\n",
       " u'news',\n",
       " u'cycle',\n",
       " u'order',\n",
       " u'panic',\n",
       " u'voter',\n",
       " u'fleeing',\n",
       " u'trump',\n",
       " u'that\\u2019s',\n",
       " u'happened',\n",
       " u'judge',\n",
       " u'incident',\n",
       " u'khan',\n",
       " u'fiasco',\n",
       " u'recently',\n",
       " u'access',\n",
       " u'hollywood',\n",
       " u'tape',\n",
       " u'trump\\u2019s',\n",
       " u'come',\n",
       " u'slam',\n",
       " u'make',\n",
       " u'story',\n",
       " u'far',\n",
       " u'bigger',\n",
       " u'deal',\n",
       " u'dynamic',\n",
       " u'it\\u2019s',\n",
       " u'true',\n",
       " u'say',\n",
       " u'happened',\n",
       " u'october',\n",
       " u'8',\n",
       " u'2016',\n",
       " u'day',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'died',\n",
       " u'day',\n",
       " u'nevertrump',\n",
       " u'coup',\n",
       " u'republican',\n",
       " u'senate',\n",
       " u'14',\n",
       " u'elected',\n",
       " u'republican',\n",
       " u'publicly',\n",
       " u'abandoned',\n",
       " u'him',\n",
       " u'historically',\n",
       " u'unprecedented',\n",
       " u'betrayal',\n",
       " u'\\u2013',\n",
       " u'never',\n",
       " u'american',\n",
       " u'history',\n",
       " u'even',\n",
       " u'bull',\n",
       " u'party',\n",
       " u'elected',\n",
       " u'member',\n",
       " u'one',\n",
       " u'party',\n",
       " u'nominee',\n",
       " u'utterly',\n",
       " u'predictable',\n",
       " u'result',\n",
       " u'nevertrump',\n",
       " u'coup',\n",
       " u'party',\n",
       " u'split',\n",
       " u'showed',\n",
       " u'poll',\n",
       " u'event',\n",
       " u'altered',\n",
       " u'whole',\n",
       " u'trajectory',\n",
       " u'tone',\n",
       " u'race',\n",
       " u'would',\n",
       " u'happened',\n",
       " u'pat',\n",
       " u'buchanan',\n",
       " u'ron',\n",
       " u'paul',\n",
       " u'mike',\n",
       " u'huckabee',\n",
       " u'beaten',\n",
       " u'dole',\n",
       " u'1996',\n",
       " u'romney',\n",
       " u'2012',\n",
       " u'believe',\n",
       " u'result',\n",
       " u'would',\n",
       " u'think',\n",
       " u'republican',\n",
       " u'establishment',\n",
       " u'would',\n",
       " u'joined',\n",
       " u'force',\n",
       " u'democratic',\n",
       " u'establishment',\n",
       " u'order',\n",
       " u'preserve',\n",
       " u'neoliberal',\n",
       " u'globalist',\n",
       " u'status',\n",
       " u'quo',\n",
       " u'would',\n",
       " u'kind',\n",
       " u'campaign',\n",
       " u'republican',\n",
       " u'nomination',\n",
       " u'would',\n",
       " u'similar',\n",
       " u'fight',\n",
       " u'establishment',\n",
       " u'candidate',\n",
       " u'loses',\n",
       " u'primary',\n",
       " u'republican',\n",
       " u'party',\n",
       " u'always',\n",
       " u'risk',\n",
       " u'trump',\n",
       " u'loses',\n",
       " u'landslide',\n",
       " u'response',\n",
       " u'gop',\n",
       " u'establishment',\n",
       " u'future',\n",
       " u'insurgent',\n",
       " u'candidate',\n",
       " u'precedent',\n",
       " u'set',\n",
       " u'2016',\n",
       " u'sabotage',\n",
       " u'nominee',\n",
       " u'order',\n",
       " u'elect',\n",
       " u'democrat',\n",
       " u'hang',\n",
       " u'power',\n",
       " u'this',\n",
       " u'impossible',\n",
       " u'reform',\n",
       " u'gop',\n",
       " u'primary',\n",
       " u'system',\n",
       " u'yes',\n",
       " u'must',\n",
       " u'face',\n",
       " u'fact',\n",
       " u'fight',\n",
       " u'out\\u201d',\n",
       " u'beyond',\n",
       " u'november',\n",
       " u'conservatism',\n",
       " u'inc',\n",
       " u'thrown',\n",
       " u'either',\n",
       " u'support',\n",
       " u'2020',\n",
       " u'rubio',\n",
       " u'2020',\n",
       " u'candidate',\n",
       " u'minority',\n",
       " u'neoliberal',\n",
       " u'globalist',\n",
       " u'else',\n",
       " u'brad',\n",
       " u'griffin',\n",
       " u'editor',\n",
       " u'dissent',\n",
       " u'writes',\n",
       " u'pen',\n",
       " u'name',\n",
       " u'hunter',\n",
       " u'wallace',\n",
       " u'reprinted',\n",
       " u'vdarecom',\n",
       " u'permission',\n",
       " u'author',\n",
       " u'representative']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean_freqs]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 3),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 2),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 2),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 5),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 2),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 2),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 2),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 2),\n",
       " (161, 1),\n",
       " (162, 3),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 1),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 2),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 2),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 3),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 2),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 1),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 2),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 5),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 2),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (335, 2),\n",
       " (336, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 1),\n",
       " (342, 1),\n",
       " (343, 1),\n",
       " (344, 2),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 2),\n",
       " (348, 1),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 2),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 2),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 1),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 1),\n",
       " (374, 1),\n",
       " (375, 1),\n",
       " (376, 2),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 1),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 7),\n",
       " (390, 1),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 1),\n",
       " (396, 2),\n",
       " (397, 1),\n",
       " (398, 1),\n",
       " (399, 1),\n",
       " (400, 1),\n",
       " (401, 1),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 1),\n",
       " (410, 1),\n",
       " (411, 1),\n",
       " (412, 1),\n",
       " (413, 1),\n",
       " (414, 1),\n",
       " (415, 1),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 1),\n",
       " (435, 1),\n",
       " (436, 1),\n",
       " (437, 1),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 1),\n",
       " (441, 1),\n",
       " (442, 1),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 1),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 5),\n",
       " (450, 1),\n",
       " (451, 1),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 1),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 1),\n",
       " (458, 1),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 1),\n",
       " (462, 1),\n",
       " (463, 1),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 2),\n",
       " (467, 1),\n",
       " (468, 8),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 1),\n",
       " (472, 1),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 1),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 1),\n",
       " (482, 1),\n",
       " (483, 1),\n",
       " (484, 1),\n",
       " (485, 1),\n",
       " (486, 1),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 1),\n",
       " (490, 1),\n",
       " (491, 1),\n",
       " (492, 1),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 2),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 1),\n",
       " (506, 1),\n",
       " (507, 1),\n",
       " (508, 1),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 1),\n",
       " (512, 1),\n",
       " (513, 1),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 1),\n",
       " (521, 2),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 1),\n",
       " (531, 1),\n",
       " (532, 1),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 1),\n",
       " (538, 1),\n",
       " (539, 1),\n",
       " (540, 1),\n",
       " (541, 1),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 1),\n",
       " (548, 1),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 1),\n",
       " (555, 1),\n",
       " (556, 1),\n",
       " (557, 1),\n",
       " (558, 1),\n",
       " (559, 1),\n",
       " (560, 1),\n",
       " (561, 1),\n",
       " (562, 2),\n",
       " (563, 1),\n",
       " (564, 1),\n",
       " (565, 1),\n",
       " (566, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 1),\n",
       " (570, 1),\n",
       " (571, 1),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 1),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 1),\n",
       " (581, 1),\n",
       " (582, 1),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 1),\n",
       " (586, 1),\n",
       " (587, 1),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 1),\n",
       " (592, 1),\n",
       " (593, 2),\n",
       " (594, 1),\n",
       " (595, 1),\n",
       " (596, 1),\n",
       " (597, 1),\n",
       " (598, 2),\n",
       " (599, 1),\n",
       " (600, 1),\n",
       " (601, 1),\n",
       " (602, 1),\n",
       " (603, 1),\n",
       " (604, 1),\n",
       " (605, 1),\n",
       " (606, 1),\n",
       " (607, 1),\n",
       " (608, 1),\n",
       " (609, 1),\n",
       " (610, 1),\n",
       " (611, 1),\n",
       " (612, 1),\n",
       " (613, 1),\n",
       " (614, 1),\n",
       " (615, 1),\n",
       " (616, 1),\n",
       " (617, 1),\n",
       " (618, 1),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 1),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "num_topics = 100\n",
    "chunksize = 300\n",
    "\n",
    "\n",
    "# low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "# low eta means each topic is only represented by a small number of words, and vice versa\n",
    "\n",
    "ldamodel = Lda(\n",
    "    doc_term_matrix, \n",
    "    num_topics=num_topics, \n",
    "    id2word=dictionary, \n",
    "    alpha=1e-2, \n",
    "    eta=0.5e-2, \n",
    "    chunksize=chunksize, \n",
    "    minimum_probability=0.0, \n",
    "    passes=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "ldamodel.save('fake.bigrams')\n",
    "\n",
    "#Load model\n",
    "# ldamodel = Lda.load('fake.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ldamodel.print_topics(num_topics=100, num_words=5):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topic(topicid=4, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VK walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40328\n"
     ]
    }
   ],
   "source": [
    "def get_res_arr(filename):\n",
    "    res_arr = []\n",
    "    data = json.load(open(filename))\n",
    "    for id in data:\n",
    "        res_arr.append(data[id])\n",
    "    #print(len(res_arr))\n",
    "    return res_arr\n",
    "\n",
    "path='./users_posts/'\n",
    "super_arr = []\n",
    "for filename in os.listdir(path):\n",
    "    #print(filename)\n",
    "    super_arr.extend(get_res_arr(path + filename))\n",
    "\n",
    "print(len(super_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('russian') + stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    #print(doc)\n",
    "    doc = str(doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in super_arr]   \n",
    "print(doc_clean[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel.save('vk.unigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_term_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-aadab425ea72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running and Trainign LDA model on the document term matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mldamodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_term_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel.save('vk.bigrams')\n",
    "ldamodel = Lda.load('vk.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009*\"с_днем\" + 0.009*\"днем_рождения\" + 0.007*\"картинки_открытки\" + 0.007*\"открытки_♥\" + 0.007*\"с_днём\" + 0.006*\"днём_рождения\" + 0.005*\"новая_открытка\" + 0.005*\"открытки_поздравления\" + 0.005*\"😉_открытки\" + 0.005*\"поздравления_😉\"\n",
      "0.008*\"ᅠ_ᅠ\" + 0.006*\"n╬═╬_n╬═╬\" + 0.003*\"спасибо_помощь\" + 0.003*\"от_души\" + 0.003*\"души_спасибо\" + 0.003*\"помощь_бою\" + 0.002*\"бою_беспредельшиком\" + 0.002*\"беспредельшиком_от\" + 0.001*\"в_наличии\" + 0.001*\"светлых_сил\"\n",
      "0.007*\"n_n\" + 0.001*\"n_●\" + 0.001*\"n_n1\" + 0.001*\"—_это\" + 0.001*\"n_n2\" + 0.001*\"новогодние_подарки\" + 0.001*\"своим_друзьям\" + 0.001*\"n_n3\" + 0.001*\"игрушек_пожалуйста\" + 0.001*\"–_это\"\n",
      "0.003*\"одержал_победу\" + 0.003*\"я_одержал\" + 0.002*\"победу_сражении\" + 0.001*\"победу_боссом\" + 0.001*\"13_13\" + 0.001*\"ответом_askfm\" + 0.001*\"поделилсялась_ответом\" + 0.001*\"я_получил\" + 0.001*\"askfm_поделилсялась\" + 0.001*\"принял_участие\"\n",
      "0.001*\"новый_год\" + 0.001*\"—_это\" + 0.001*\"люблю_люблю\" + 0.001*\"друг_друга\" + 0.001*\"каждый_день\" + 0.001*\"социальной_сети\" + 0.001*\"моей_жизни\" + 0.001*\"именно_поэтому\" + 0.001*\"самом_деле\" + 0.000*\"n_nя\"\n",
      "0.001*\"◎_◎\" + 0.000*\"первый_кликнувший\" + 0.000*\"кликнувший_получит\" + 0.000*\"ежедневной_лотерее\" + 0.000*\"выигрывай_первый\" + 0.000*\"лотерее_играй\" + 0.000*\"наноферму_выигрывай\" + 0.000*\"играй_наноферму\" + 0.000*\"награду_ежедневной\" + 0.000*\"ершик_ершик\"\n",
      "0.001*\"новый_год\" + 0.001*\"новым_годом\" + 0.001*\"2017_года\" + 0.001*\"–_это\" + 0.001*\"каждый_день\" + 0.001*\"это_очень\" + 0.001*\"спасибо_всем\" + 0.001*\"день_рождения\" + 0.001*\"сих_пор\" + 0.001*\"дорогие_друзья\"\n",
      "0.004*\"—_это\" + 0.003*\"–_это\" + 0.002*\"n_n\" + 0.001*\"каждый_день\" + 0.001*\"n_nи\" + 0.001*\"n_nв\" + 0.001*\"самом_деле\" + 0.001*\"своей_жизни\" + 0.001*\"n_n—\" + 0.001*\"n_nя\"\n",
      "0.002*\"днём_рождения\" + 0.002*\"new_ninstagram\" + 0.002*\"ninstagram_nvk\" + 0.002*\"новым_годом\" + 0.002*\"nvk_new\" + 0.002*\"днем_рождения\" + 0.002*\"с_днём\" + 0.001*\"с_днем\" + 0.001*\"вітаю_вітаю\" + 0.001*\"задай_вопрос\"\n",
      "0.002*\"♥_♥\" + 0.002*\"of_the\" + 0.002*\"in_the\" + 0.002*\"ярик_лапа\" + 0.001*\"moscow_russia\" + 0.001*\"for_the\" + 0.001*\"певец_песни\" + 0.001*\"on_the\" + 0.001*\"i_am\" + 0.001*\"to_be\"\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_term_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-305c2699fba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_term_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(word for d in doc_clean for word in d)\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_words = dict(top_k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_freqs = [[w for w in doc if w in top_k_words] for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean_freqs]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "num_topics = 100\n",
    "chunksize = 300\n",
    "\n",
    "\n",
    "# low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "# low eta means each topic is only represented by a small number of words, and vice versa\n",
    "\n",
    "ldamodel = Lda(\n",
    "    doc_term_matrix, \n",
    "    num_topics=num_topics, \n",
    "    id2word=dictionary, \n",
    "    alpha=1e-2, \n",
    "    eta=0.5e-2, \n",
    "    chunksize=chunksize, \n",
    "    minimum_probability=0.0, \n",
    "    passes=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "ldamodel.save('fake.bigrams')\n",
    "\n",
    "#Load model\n",
    "# ldamodel = Lda.load('fake.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ldamodel.print_topics(num_topics=100, num_words=5):\n",
    "    print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topic(topicid=4, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.machinelearning.ru/wiki/images/8/82/BMMO11_14.pdf\n",
    "http://www.machinelearning.ru/wiki/images/f/f7/DirichletProcessNotes.pdf \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
