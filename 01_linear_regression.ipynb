{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pics/otus.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from itertools import izip\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas.tools.plotting import table\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи машинного обучения\n",
    "* Обучение с учителем\n",
    "    * Регрессия\n",
    "    * Классификация\n",
    "* Обучение без учителя\n",
    "    * Кластеризация\n",
    "    * Снижение размерности\n",
    "* Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение с учителем** - есть некоторое количество примеров, для которых известны ответы.\n",
    "* ответы числа - регрессия\n",
    "* ответы классы - классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].scatter(x, y)\n",
    "ax[0].set_title(\"Regression problem \\n What is y value for new x?\")\n",
    "ax[0].set_xlabel(\"x (feature)\")\n",
    "ax[0].set_ylabel(\"y (target)\")\n",
    "\n",
    "x1 = 2 * np.random.rand(100, 1)\n",
    "x2 = 4 + 3 * np.random.randn(100, 1)\n",
    "\n",
    "ax[1].scatter(x1[x1 > 1], x2[x1 > 1], label=\"class 1\", c='r')\n",
    "ax[1].scatter(x1[x1 <= 1], x2[x1 <= 1], label=\"class 2\", c='b')\n",
    "ax[1].set_title(\"Classification problem \\n What is the color for the new (x1, x2) pair?\")\n",
    "ax[1].set_xlabel(\"x1 (feature)\")\n",
    "ax[1].set_ylabel(\"x2 (feature)\")\n",
    "ax[1].legend()\n",
    "fig.savefig(\"pics/supervised.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без учителя - нет ответов\n",
    "* хотим разделить данные на группы, но не знаем, есть ли они, сколько их, что они означают\n",
    "* хотим отделить шум от остальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "data = [noisy_circles, noisy_moons, blobs, no_structure]\n",
    "for i_dataset, dataset in enumerate(data):\n",
    "    x, y = dataset\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    ax_i = ax[i_dataset / 2][i_dataset % 2]\n",
    "    ax_i.scatter(x[:, 0], x[:, 1])\n",
    "    ax_i.set_xlim(-2, 2)\n",
    "    ax_i.set_ylim(-2, 2)\n",
    "    ax_i.set_xlabel(\"x1 (feature)\")\n",
    "    ax_i.set_ylabel(\"x2 (feature)\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"pics/unsupervised.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С подкреплением - реакция на обратную связь для максимизации выгоды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С учителем - регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1)\n",
    "df = pd.DataFrame(np.hstack([x, y]), columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('x', 'y')\n",
    "plt.title(\"Regression problem \\n What is y value for new x?\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.savefig('pics/regression.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парная регрессия\n",
    "\n",
    "### Описание данных\n",
    "$$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n",
    "\n",
    "### Предсказание \n",
    "$$ \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим случайную прямую. Насколько хорошо она описывает данные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    return 2 * x + 5\n",
    "\n",
    "df.plot.scatter('x', 'y')\n",
    "plt.title(\"Regression problem \\n How good is red line?\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.plot(x, predict(x), color='red', label='prediction')\n",
    "plt.legend()\n",
    "plt.savefig('pics/regression_random_line.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS\n",
    "Residual Sum of Squares\n",
    "\n",
    "$$ RSS = \\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\rightarrow min$$\n",
    "\n",
    "Метод наименьших квадратов.\n",
    "Эта метрика была выбрана как функция, которую легко было минимизировать.  \n",
    "После было доказано (теорема Гаусса Маркова), что она дает оптимальное решение (при определенных условиях, накладываемых на данные).  \n",
    "Более подробно об этом можно узнать, например, в курсе \"Эконометрика\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(x)\n",
    "\n",
    "df.plot.scatter('x', 'y')\n",
    "plt.title(\"Regression problem \\n How good is red line?\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.plot(x, y_pred, color='red', label='prediction')\n",
    "plt.legend()\n",
    "\n",
    "for x_i, y_i, y_i_pred in izip(x, y, y_pred):\n",
    "    plt.plot([x_i, x_i], [y_i_pred, y_i], color='red', alpha=0.5)\n",
    "\n",
    "plt.savefig('pics/regression_random_line_mse.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss = ((y - y_pred) ** 2).sum()\n",
    "print rss\n",
    "\n",
    "print mean_squared_error(y, y_pred) * y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Минимизация - $\\beta_0$\n",
    "\n",
    "$$ RSS = \\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\rightarrow min$$\n",
    "\n",
    "$$  \\frac{d\\mathcal{L}}{d\\beta_0} =  \n",
    "\\frac{d}{d\\beta_0}(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2) = \n",
    "\\frac{d}{d\\beta_0}(\\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) ^ 2) =  \n",
    "-2\\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -\\sum_{i=1}^{n}y_i +\\sum_{i=1}^{n} \\hat{\\beta_0} +\\sum_{i=1}^{n} \\hat{\\beta_1}x_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ n\\hat{\\beta}_0 = \\sum_{i=1}^{n}y_i - \\sum_{i=1}^{n} \\hat{\\beta_1}x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta}_0 = \\frac{ \\sum_{i=1}^{n}y_i}{n} - \\frac{\\sum_{i=1}^{n} \\hat{\\beta_1}x_i}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замечание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\overline{y} = \\frac{ \\sum_{i=1}^{n}y_i}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\overline{x} = \\frac{ \\sum_{i=1}^{n}x_i}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому \n",
    "\n",
    "$$ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta_1}\\overline{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Минимизация - $\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RSS = \\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\rightarrow min$$\n",
    "\n",
    "$$  \\frac{d\\mathcal{L}}{d\\beta_1} =  \n",
    "\\frac{d}{d\\beta_1}(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2) = \n",
    "\\frac{d}{d\\beta_1}(\\sum_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) ^ 2) =  \n",
    "-2\\sum_{i=1}^{n}x_i(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подставим $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-2\\sum_{i=1}^{n}x_i(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{n}(x_iy_i - x_i\\overline{y} + x_i\\hat{\\beta_1}\\overline{x} - x_i\\hat{\\beta_1}x_i) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{n}((x_i y_i - x_i\\overline{y}) + \\hat{\\beta_1}(x_i\\overline{x} - x_i^2)) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{n}(x_iy_i - x_i\\overline{y}) + \\sum_{i=1}^{n}\\hat{\\beta_1}(x_i\\overline{x} - x_i^2) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta_1}\\sum_{i=1}^{n}(x_i^2 - x_i\\overline{x}) = \\sum_{i=1}^{n}(x_iy_i - x_i\\overline{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta_1} = \\frac{ \\sum_{i=1}^{n}(x_iy_i - x_i\\overline{y})}{\\sum_{i=1}^{n}(x_i^2 - x_i\\overline{x})} =\n",
    "\\frac{\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\overline{y}}{\\sum_{i=1}^{n}x_i^2 - \\sum_{i=1}^{n}x_i\\overline{x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Замечания\n",
    "\n",
    "$$ n\\overline{x}\\overline{y} = \\sum_{i=1}^{n}x_i\\overline{y} = \\sum_{i=1}^{n}y_i\\overline{x} = \\sum_{i=1}^{n}\\overline{y}\\overline{x}  $$\n",
    "\n",
    "$$\\sum_{i=1}^{n}x_i\\overline{x} = \\frac{n\\overline{x}\\sum_{i=1}^{n}x_i}{n} = n\\overline{x}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому\n",
    "\n",
    "Числитель\n",
    "\n",
    "$$ \\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\overline{y} = \\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\overline{y} + n\\overline{x}\\overline{y} - n\\overline{x}\\overline{y} =\n",
    "\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\overline{y} + \\sum_{i=1}^{n}\\overline{x}\\overline{y} - \\sum_{i=1}^{n}\\overline{x}y_i = \n",
    "\\sum_{i=1}^{n} (x_iy_i - x_i\\overline{y} + \\overline{x}\\overline{y} - \\overline{x}y_i) =\n",
    "\\sum_{i=1}^{n}((x_i - \\overline{x}) (y_i - \\overline{y}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знаменатель\n",
    "\n",
    "$$\\sum_{i=1}^{n}x_i^2 - \\sum_{i=1}^{n}x_i\\overline{x} = \n",
    "\\sum_{i=1}^{n}(x_i^2 - x_i\\overline{x}) =\n",
    "\\sum_{i=1}^{n}(x_i^2 - x_i\\overline{x} - x_i\\overline{x} + x_i\\overline{x}) =\n",
    "\\sum_{i=1}^{n}(x_i^2 - 2x_i\\overline{x} + x_i\\overline{x}) = \n",
    "\\sum_{i=1}^{n}(x_i^2 - 2x_i\\overline{x} + \\overline{x}^2) = \n",
    "\\sum_{i=1}^{n}(x_i - \\overline{x})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В итоге\n",
    "\n",
    "$$ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta_1}\\overline{x} $$\n",
    "\n",
    "$$ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}((x_i - \\overline{x}) (y_i - \\overline{y}))}{\\sum_{i=1}^{n}(x_i - \\overline{x})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate(x, y):\n",
    "    y_av = y.mean()\n",
    "    x_av = x.mean()\n",
    "    \n",
    "    b1 = ((x - x_av) * (y - y_av)).sum() / ((x - x_av) ** 2).sum()\n",
    "    b0 = y_av - b1 * x_av\n",
    "    \n",
    "    return b0, b1\n",
    "\n",
    "def predict_estimated(x, b0, b1):\n",
    "    return b0 + x * b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0, b1 = estimate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('x', 'y')\n",
    "plt.title(\"Regression problem\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.plot(x, predict(x), color='red', label='prediction')\n",
    "plt.plot(x, predict_estimated(x, 4, 3), color='blue', label='prediction initial')\n",
    "plt.plot(x, predict_estimated(x, b0, b1), color='green', label='prediction estimated')\n",
    "plt.legend()\n",
    "plt.savefig('pics/regression_estimated.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"estimated values: b0 = {}, b1 = {}\".format(b0, b1)\n",
    "print \"initial values: b0 = 4, b1 = 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = np.hstack([x, np.ones(x.shape[0]).reshape(-1, 1)])\n",
    "m, c = np.linalg.lstsq(x_dummy, y)[0]\n",
    "print m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Множественная регрессия\n",
    "\n",
    "### Предсказание \n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_p x_p $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]\n",
    "fig=plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "x1 = 2 * np.random.rand(100, 1)\n",
    "x2 = 5 * np.random.rand(100, 1)\n",
    "\n",
    "# y = 4 + 3*x_1 + 1*x_2\n",
    "y = 4 + 3 * x1 + x2 + np.random.randn(100, 1)\n",
    "\n",
    "ax.scatter(x1, x2, y, marker='o')\n",
    "ax.set_title(\"Regression problem \\n What is y value for new (x1, x2)?\")\n",
    "ax.set_xlabel(\"x1 (feature)\")\n",
    "ax.set_ylabel(\"x2 (feature)\")\n",
    "ax.set_zlabel(\"y (target)\")\n",
    "plt.savefig('pics/regression_3d.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричная форма "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем $x_0$ - векктор единиц:\n",
    "\n",
    "$$ x = \\begin{bmatrix} 1 & x_{11} & ... & x_{p1} \\\\ 1 & x_{12} & ... & x_{p2} \\\\ ... & ... & ... & ... \\\\ 1 & x_{n1} & ... & x_{n1}\n",
    "\\end{bmatrix} \\quad \n",
    "\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ ... \\\\ \\theta_p \\end{bmatrix} \\quad \n",
    "y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x\\theta = y$$\n",
    "\n",
    "$$x_{n\\cdot p}\\cdot\\theta_{p\\cdot 1} = y_{n\\cdot 1} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RSS = \\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^n (y_i - x_i^T\\theta)^2 = \\\\= \n",
    "(y_1 - x_1^T\\theta)^2 + (y_2 - x_2^T\\theta)^2 + ... + (y_n - x_n^T\\theta)^2 = \\\\\n",
    "=[y_1 - x_1^T\\theta, y_2 - x_2^T\\theta, ..., y_n - x_n^T\\theta]\n",
    "\\begin{bmatrix} y_1 - x_1^T\\theta \\\\ y_2 - x_2^T\\theta \\\\ ... \\\\ y_n - x_n^T\\theta \\end{bmatrix} = \\\\\n",
    "= (y-x\\theta)^T(y-x\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Производная по матрице\n",
    "\n",
    "http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf  \n",
    "http://nabatchikov.com/blog/view/matrix_der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Здесь под производной скалярной функции $f(x)$ по вектору x понимается градиент\n",
    "$$ \\frac{df(x)}{dx} = [ \\frac{df(x)}{dx_1}, . . . , \\frac{df(x)}{dx_d}]^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свойства, которые понадобятся далее:\n",
    "\n",
    "$$\\frac{dx^TA}{dx} = \\frac{dA^Tx}{dx} = A$$\n",
    "\n",
    "$$\\frac{dx^TAx}{dx} = 2Ax$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приравниваем производную к нулю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\mathcal{L}}{d\\theta} = \\frac{d}{d\\theta} (y-x\\theta)^T(y-x\\theta) = \\\\\n",
    "= \\frac{d}{d\\theta} (y^T-(x\\theta)^T)(y-x\\theta) = \\\\ \n",
    "= \\frac{d}{d\\theta}( (y^T-(x\\theta)^T)y - (y^T-(x\\theta)^T)(x\\theta)) = \\\\ \n",
    "= \\frac{d}{d\\theta}(y^Ty-(x\\theta)^Ty - y^Tx\\theta + (x\\theta)^Tx\\theta) = \\\\ \n",
    "= \\frac{d}{d\\theta}(y^Ty-\\theta^Tx^Ty - y^Tx\\theta + \\theta^Tx^Tx\\theta) = \\\\ \n",
    "= \\frac{d}{d\\theta}(y^Ty)-\\frac{d}{d\\theta}(\\theta^T(x^Ty)) - \\frac{d}{d\\theta}((y^Tx)\\theta) + \\frac{d}{d\\theta}(\\theta^T(x^Tx)\\theta) = \\\\ \n",
    "= 0-x^Ty - x^Ty + 2x^Tx\\theta = -2x^Ty + 2x^Tx\\theta = 0\\\\ \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^Tx\\theta=x^Ty$$\n",
    "$$(x^Tx)^{-1}x^Tx\\theta=(x^Tx)^{-1}x^Ty$$\n",
    "$$\\theta=(x^Tx)^{-1}x^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack([np.ones(x1.shape[0]).reshape(-1, 1), x1, x2])\n",
    "\n",
    "w = la.inv(x.T.dot(x)).dot(x.T).dot(y) \n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_estimated(x, w):\n",
    "    return x.dot(w)\n",
    "\n",
    "def plane(x1, x2):\n",
    "    return 4 + x1 * 3 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig=plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "x1_surfs = np.arange(0, 2.5, 0.5)\n",
    "x2_surfs = np.arange(0, 5.0, 1)\n",
    "x1_surf, x2_surf = np.meshgrid(x1_surfs, x2_surfs)\n",
    "\n",
    "zs = np.array([plane(x_i, y_i) for x_i, y_i in zip(np.ravel(x1_surf), np.ravel(x2_surf))])\n",
    "z = zs.reshape(x1_surf.shape)\n",
    "ax.plot_surface(x1_surf, x2_surf, z, alpha=0.2)\n",
    "\n",
    "zs_pred = np.array([predict_estimated(np.array([[1, x_i, y_i]]), w) for x_i, y_i in zip(np.ravel(x1_surf), np.ravel(x2_surf))])\n",
    "z_predicted = zs_pred.reshape(x1_surf.shape)\n",
    "ax.plot_surface(x1_surf, x2_surf, z_predicted, alpha=0.2, color='red')\n",
    "ax.scatter(x1, x2, y, marker='o')\n",
    "\n",
    "ax.set_title(\"Regression problem \\n What is y value for new (x1, x2)?\")\n",
    "ax.set_xlabel(\"x1 (feature)\")\n",
    "ax.set_ylabel(\"x2 (feature)\")\n",
    "ax.set_zlabel(\"y (target)\")\n",
    "plt.savefig('pics/regression_3d_estimated.pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(np.hstack([x1, x2]), y)\n",
    "print lin_reg.intercept_, lin_reg.coef_\n",
    "y_pred = lin_reg.predict(np.hstack([x1, x2]))\n",
    "print np.hstack([y_pred, y])[:10]\n",
    "print mean_squared_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычислительная сложность \n",
    "\n",
    "Вычисляется матрица $(x^Tx)^{-1}$ размера $p \\times p$, где $p$ - количество признаков (feature). \n",
    "Вычислитальная сложность обратной матрицы обычно от $O(n^{2.4})$ до $O(n^3)$\n",
    "\n",
    "При удвоении количества признаков время на вычисления нужно умножить на от $2^{2.4} = 5.3$ до $2^3 = 8$.\n",
    "\n",
    "Рассчет становится очень медленным, когда количество признаков очень большое - например, 100,000\n",
    "\n",
    "При этом рассчет линеен по отношению к количеству элементов выборки ($O(n)$). Т.о. можно эффективно обрабатывать большие наборы данных, если они помещаются в оперативную память.\n",
    "\n",
    "При этом предсказание очень быстрое и зависит только от количества объектов, по которым необходимо рассчитать значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полиномиальная регрессия\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.title(\"Regression problem \\n What is y value for new x?\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.savefig('pics/regression_poly.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширим пространство гипотез до всех полиномов степени $p$. \n",
    "\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_p x_p + \\theta_{12} x_1^2 + \\theta_{22} x_1x_2 + ... + \\theta_{p2} x_1x_p + ... + \\theta_{pp} x_p^2 + ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"g-\", linewidth=2, label=\"Predictions\")\n",
    "plt.title(\"Regression problem \\n What is y value for new x?\")\n",
    "plt.xlabel(\"x (feature)\")\n",
    "plt.ylabel(\"y (target)\")\n",
    "plt.savefig('pics/regression_poly_predicted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.savefig('pics/regression_poly_overfit.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация\n",
    "\n",
    "$$ RSS = \\mathcal{L}(y, \\theta, x) = \\sum_{i=1}^{n}(y_i - x_i^T\\theta)^2 + \\alpha R(\\theta) \\rightarrow min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Гребневая регрессия (Ridge regression, регуляризация Тихонова, $L^2$-регуляризация)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ RSS = \\mathcal{L}(y, \\theta, x) = \\sum_{i=1}^{n}(y_i - x_i^T\\theta)^2 + \\alpha \\sum_{i=1}^{n} \\theta_i^2 \\rightarrow min $$\n",
    "\n",
    "### Замечание \n",
    "$\\theta_0$ не участвует в регрессии, так как нужна возможность провести гиперплоскость с любым сдвигом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова будем использовать свойство\n",
    "\n",
    "$$\\frac{dx^TAx}{dx} = 2Ax$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\mathcal{L}}{d\\theta} = \\frac{d}{d\\theta} (y-x\\theta)^T(y-x\\theta) +  \\frac{d}{d\\theta} \\alpha\\theta^T\\theta = \\\\\n",
    "= -2x^Ty + 2x^Tx\\theta + 2\\alpha \\theta = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x^Tx\\theta + \\alpha \\theta = x^Ty $$\n",
    "$$ (x^Tx + \\alpha E) \\theta = x^Ty $$\n",
    "\n",
    "$E$ - единичная диагональная матрица\n",
    "\n",
    "$$ (x^Tx + \\alpha E)^{-1} (x^Tx + \\alpha E)\\theta = (x^Tx + \\alpha E)^{-1} x^Ty $$\n",
    "\n",
    "$$ \\theta = (x^Tx + \\alpha E)^{-1} x^Ty $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замечание\n",
    "\n",
    "Гребнем является диагональная матрица, которую мы прибавляем к матрице $X^TX$ с линейнозависимыми колонками, в результате получаемая матрица не сингулярна (не имеет линейно зависимых строк и столбцов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лассо регрессия (Least Absolute Shrinkage and Selection Operator Regression, Lasso regression, $L^1$-регуляризация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RSS = \\mathcal{L}(y, \\theta, x) = \\sum_{i=1}^{n}(y_i - x_i^T\\theta)^2 + \\alpha \\sum_{i=1}^{n} |\\theta_i| \\rightarrow min $$\n",
    "\n",
    "Сумма модулей весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замечание\n",
    "\n",
    "$$|x|' = \\frac{x}{|x|} = sign(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\frac {d\\mathcal{L}}{d\\theta_i} = 2 \\sum_{i=1}^{n} ((y_i - x_i^T\\theta)x_i) + \\alpha sign(\\theta_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая задача не имеет решения в явном виде, для ее решения будем использовать градиентный спуск.\n",
    "Важно делать нормализацию признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замечание \n",
    "Важная характеристика Лассо регрессии - устранение наименее важных признаков (обнуление их весов).\n",
    "Происходит автоматический выбор признаков и получается разреженная модель (несколько ненулевых весов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "$$ RSS = \\mathcal{L}(y, \\theta, x) = \\sum_{i=1}^{n}(y_i - x_i^T\\theta)^2 +r \\alpha \\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2 \\rightarrow min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всегда следует делать регуляризацию. То есть следует избегать обычной линейной регрессии. \n",
    "Если есть знание, что часть признаков не важны, то используется Lasso или Elastic Net. \n",
    "Если признаки скоррелированы или количество признаков больше количества примеров, то лучше использовать Elastic Net или Ridge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habrahabr.ru/company/ods/blog/322076/  \n",
    "Aurélien Géron - Hands-on Machine Learning with Scikit-Learn and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
